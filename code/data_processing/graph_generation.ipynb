{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from typing import List\n",
    "import datetime\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import socket\n",
    "from shutil import copyfile\n",
    "from contextlib import contextmanager\n",
    "import matplotlib.ticker as ticker\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import collections\n",
    "import json\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils ###\n",
    "\n",
    "# Suppress Console Output\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "# Encoder For TypeCasting Numpy Formats in DataFrame\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "# Save Graph in JSON Format\n",
    "def save_graph_json(path, graph):\n",
    "    data = json_graph.node_link_data(graph)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, cls=MyEncoder)\n",
    "\n",
    "\n",
    "# Import Graph from JSON Format\n",
    "def import_graph_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    contents = json.loads(data)\n",
    "    return json_graph.node_link_graph(contents)\n",
    "\n",
    "\n",
    "# Get Time Sorted CSV List\n",
    "def get_csv_list(csv_dir):\n",
    "    # Capture Files in Time Sorted List \n",
    "    csv_files = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(csv_dir, \"*.csv\"))):\n",
    "        csv_files.append(file)\n",
    "\n",
    "    # Return \n",
    "    return csv_files\n",
    "\n",
    "\n",
    "# Remove Unwanted Protocols from DataFrame\n",
    "def filter_protocols(df):\n",
    "    # Dump Protocols\n",
    "    dump = ['IrDA', 'USB', 'DSL', 'ISDN', 'ITU', 'ARINC', 'Ethernet', 'Bluetooth', 'ARCnet', 'ARP', 'ATM', 'CHAP', 'CDP', 'DCAP', 'DTP', 'Econet', 'FDDI', 'ITU-T', 'HDLC', 'IEEE 802.11', 'IEEE 802.16', 'LACP', 'LattisNet', 'LocalTalk', 'L2F', 'L2TP', 'LLDP', 'MAC', 'Q.710', 'NDP', 'PAgP', 'PPP', 'PPTP', 'PAP', 'RPR', 'SLIP', 'StarLAN', 'STP', 'Token Ring', 'VTP', 'VEN', 'VLAN', 'ATM', 'IS-IS', 'SPB', 'MTP', 'NSP', 'ARP', 'MPLS', 'PPPoE', 'TIPC', 'CLNP', 'IPX', 'NAT', 'Routed-SMLT', 'SCCP', 'HSRP', 'VRRP', 'IP', 'IPv4', 'IPv6', 'ICMP', 'ARP', 'RIP', 'OSPF', 'IPSEC', 'AppleTalk', 'DECnet', 'IPX', 'SPX', 'IGMP', 'IPsec']\n",
    "\n",
    "    # Mask Entries and Remove Unwanted Protocols From DataFrame\n",
    "    mask = df['Protocol'].apply(lambda x: any(item for item in dump if item in x))\n",
    "    df.drop(df[mask].index, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract Source Services by Port\n",
    "def src_services(df):\n",
    "     counter = 1\n",
    "     for port in df['Masked Source Port'].unique():\n",
    "         if int(port) == 66000:\n",
    "             continue\n",
    "         else:\n",
    "             try:\n",
    "                 print(\"{} {}\".format(int(port), socket.getservbyport(int(port))))\n",
    "                 counter += 1\n",
    "             except:\n",
    "                 continue\n",
    "\n",
    "\n",
    "# Extract Destination Services by Port\n",
    "def dst_services(df):\n",
    "    counter = 1\n",
    "    for port in df['Masked Destination Port'].unique():\n",
    "         if int(port) == 66000:\n",
    "             continue\n",
    "         else:\n",
    "             try:\n",
    "                 print(\"{} {}\".format(int(port), socket.getservbyport(int(port))))\n",
    "                 counter += 1\n",
    "             except:\n",
    "                 continue \n",
    "\n",
    "\n",
    "# Extract Traffic Info by Service\n",
    "def traffic_by_service(df):\n",
    "    counter = 1\n",
    "    i = 0\n",
    "    for port in df.loc[:, 'Destination Port']:\n",
    "        try:\n",
    "            df.loc[i, 'Destination Port'] = socket.getservbyport(int(port))\n",
    "            counter += 1\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "    \n",
    "    return df.loc[:, 'Destination Port']\n",
    "\n",
    "\n",
    "# Get Complete Data Set Time Info\n",
    "def get_time_info(csv_dir):\n",
    "    # Locate CSV Files\n",
    "    csv_files = get_csv_list(csv_dir)\n",
    "\n",
    "    # Extract First and Last Element\n",
    "    df_start = pd.read_csv(csv_files[0])\n",
    "    df_end = pd.read_csv(csv_files[-1])\n",
    "\n",
    "    # Extract Start and End Time Info\n",
    "    start_epoch = df_start.loc[0, \"Time Epoch\"]\n",
    "    end_epoch = df_end.loc[len(df_end)-1, \"Time Epoch\"]\n",
    "\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_time = reference + datetime.timedelta(0, start_epoch)\n",
    "    end_time = reference + datetime.timedelta(0, end_epoch)\n",
    "\n",
    "    # Extract Trace Duration\n",
    "    duration = end_epoch - start_epoch\n",
    "    d = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=int(duration))\n",
    "\n",
    "    # Print\n",
    "    print(\"Start of Trace: \", start_time)\n",
    "    print(\"End of Trace: \", end_time)\n",
    "    print(\"Total Duration of Traces: \")\n",
    "    print(\"{} Days {} Hours {} Minutes {} Seconds\".format(d.day-1, d.hour, d.minute, d.second))\n",
    "\n",
    "    # Return\n",
    "    return start_time, end_time, duration, start_epoch, end_epoch\n",
    "\n",
    "\n",
    "# Get Complete Trace Time Info\n",
    "def get_time_info_old(df):\n",
    "    # Extract Start and End Epochs\n",
    "    start_epoch = df.loc[0, \"Time Epoch\"]\n",
    "    end_epoch = df.loc[len(df)-1, \"Time Epoch\"]\n",
    "\n",
    "    # Get Start and End Date\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_date = reference + datetime.timedelta(0, start_epoch)\n",
    "    end_date = reference + datetime.timedelta(0, end_epoch)\n",
    "\n",
    "    # Get Trace Duration\n",
    "    duration = end_epoch - start_epoch\n",
    "    d = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=int(duration))\n",
    "\n",
    "    # Print\n",
    "    print(\"Start of Trace: \", start_date)\n",
    "    print(\"End of Trace: \", end_date)\n",
    "    print(\"Total Duration of Traces: \")\n",
    "    print(\"{} Days {} Hours {} Minutes {} Seconds\".format(d.day-1, d.hour, d.minute, d.second))\n",
    "\n",
    "    # Return\n",
    "    return start_date, end_date, duration, start_epoch, end_epoch\n",
    "\n",
    "\n",
    "# Get Time Epoch Info from DateTime Object\n",
    "def get_epoch_info(start, end):\n",
    "    # Get Start and End Epoch\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_epoch = (start - reference).total_seconds()\n",
    "    end_epoch = (end - reference).total_seconds()\n",
    "\n",
    "    # Get Duration\n",
    "    duration = end_epoch - start_epoch \n",
    "\n",
    "    # Return \n",
    "    return start_epoch, end_epoch, duration\n",
    "\n",
    "\n",
    "# Get DateTime Info from Time Epoch\n",
    "def get_DateTime_from_epochs(start, end):\n",
    "    # Convert Epochs into DateTime Objects\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_date = reference + datetime.timedelta(seconds = start)\n",
    "    end_date = reference + datetime.timedelta(seconds = end)\n",
    "\n",
    "    # Return\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "# Load all CSV Files in a single DataFrame\n",
    "def load_all(csv_dir):\n",
    "    # Get All CSV Files\n",
    "    csv_files = get_csv_list(csv_dir)\n",
    "\n",
    "    # Load into Pandas and Concatenate\n",
    "    df = pd.concat((pd.read_csv(file) for file in csv_files), axis=0, ignore_index = True)\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Window Slicing Function\n",
    "def time_windowing(csv_path: str, time_window: List):\n",
    "    # Load Data\n",
    "    df = load_all(csv_path)\n",
    "\n",
    "    # Get Time Info\n",
    "    with suppress_stdout():\n",
    "        start_date, end_date, trace_duration, trace_start_epoch, trace_end_epoch = get_time_info(df)\n",
    "\n",
    "    # Check if Time Window is Valid\n",
    "    window_start = time_window[0]\n",
    "    window_end = time_window[-1]\n",
    "\n",
    "    if window_start < start_date:\n",
    "        window_start = start_date\n",
    "        print(\"Adjusted Start of Time Window to: \", window_start)\n",
    "    if window_end > end_date:\n",
    "        window_end = end_date\n",
    "        print(\"Adjusted End of Time Window to: \", window_end)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    # Convert Window Start and Window End into Valid Time Epochs\n",
    "    window_start_epoch, window_end_epoch, window_duration = get_epoch_info(window_start, window_end)\n",
    "\n",
    "    # Find Indices in DataFrame that correspond to window_start_epoch and window_end_epoch by finding closest Time Epoch values\n",
    "    all_epochs = list(df['Time Epoch'].to_numpy())\n",
    "    closest_start_epoch = min(all_epochs, key=lambda x:abs(x-window_start_epoch))\n",
    "    closest_end_epoch = min(all_epochs, key=lambda x:abs(x-window_end_epoch))\n",
    "\n",
    "    loc_start = df.loc[df['Time Epoch'] == closest_start_epoch].index[0]\n",
    "    loc_end = df.loc[df['Time Epoch'] == closest_end_epoch].index[0]\n",
    "\n",
    "    locs = np.arange(loc_start, loc_end + 1)\n",
    "\n",
    "    # Construct DataFrame of Interest with given loc_start and loc_end\n",
    "    df_new = df.loc[locs]\n",
    "    df_new = df_new.reset_index()\n",
    "    del df_new['index']\n",
    "\n",
    "    # Return \n",
    "    return df_new\n",
    "\n",
    "\n",
    "# Window Slicing Function without Loading every CSV [Faster Performance]\n",
    "def time_windowing2(csv_path: str, time_window: List):\n",
    "    # Get Epochs List from CSV Path\n",
    "    csv_epochs = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(csv_path, \"*.csv\"))):\n",
    "        full_file_path = file\n",
    "        file_only = full_file_path.split(csv_dir + '/')[1]\n",
    "        epoch_only = file_only.split('-')[0]\n",
    "        csv_epochs.append(int(epoch_only))\n",
    "\n",
    "    # Get Time Info from Epochs\n",
    "    start_epoch = int(csv_epochs[0])\n",
    "    end_epoch = int(csv_epochs[-1])\n",
    "    start_date, end_date = get_DateTime_from_epochs(start_epoch, end_epoch)\n",
    "\n",
    "    # Convert Window Start and Window End into Valid Time Epochs\n",
    "    window_start = time_window[0]\n",
    "    window_end = time_window[-1]\n",
    "    window_start_epoch, window_end_epoch, window_duration = get_epoch_info(window_start, window_end)\n",
    "\n",
    "    # Check if Time Window is Valid by Converting Epochs into Integers and Comparing them with the CSV Names\n",
    "    window_start_epoch = int(window_start_epoch)\n",
    "    window_end_epoch = int(window_end_epoch)\n",
    "\n",
    "    if window_start_epoch < start_epoch:\n",
    "        window_start_epoch = start_epoch\n",
    "        print(\"Adjusted Start of Time Window to: \", start_date)\n",
    "    if window_end_epoch > end_epoch:\n",
    "        window_end_epoch = end_epoch\n",
    "        print(\"Adjusted End of Time Window to: \", end_date)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    # Find Indices in csv_epochs that correspond to window_start_epoch and window_end_epoch by finding closest Time Epoch values\n",
    "    closest_start_epoch = min(csv_epochs, key=lambda x:abs(x-window_start_epoch))\n",
    "    closest_end_epoch = min(csv_epochs, key=lambda x:abs(x-window_end_epoch))\n",
    "\n",
    "    loc_start = csv_epochs.index(closest_start_epoch)\n",
    "    loc_end = csv_epochs.index(closest_end_epoch)\n",
    "\n",
    "    # Construct DataFrame of Interest with given loc_start and loc_end\n",
    "    csv_files = get_csv_list(csv_path)\n",
    "    desired_csv_files = csv_files[loc_start:loc_end+1]\n",
    "\n",
    "    df = pd.concat((pd.read_csv(file) for file in desired_csv_files), axis=0, ignore_index = True)\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Get Time Sorted PCAP List\n",
    "def get_pcap_list(pcap_dir):\n",
    "    # Capture Files in Time Sorted List \n",
    "    pcap_files = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(pcap_dir, \"*.pcap\"))):\n",
    "        pcap_files.append(file)\n",
    "\n",
    "    # Return \n",
    "    return pcap_files\n",
    "\n",
    "\n",
    "# Merge IPv4 and IPv6 SRC and DST Columns \n",
    "def correct_ipv6(df):\n",
    "    # Locations with NaN Values\n",
    "    locs = df['ip.src'].isna()\n",
    "\n",
    "    # Replace Values\n",
    "    df.loc[locs, 'ip.src'] = df.loc[locs, 'ipv6.src']\n",
    "    df.loc[locs, 'ip.dst'] = df.loc[locs, 'ipv6.dst']\n",
    "\n",
    "    # Delete ipv6 columns\n",
    "    del df['ipv6.src']\n",
    "    del df['ipv6.dst']\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Merge UDP and TCP SRC and DST Ports\n",
    "def correct_UDP_TCP_ports(df):\n",
    "    # Locations with NaN Values\n",
    "    locs = df['udp.srcport'].isna()\n",
    "\n",
    "    # Replace Values\n",
    "    df.loc[locs, 'udp.srcport'] = df.loc[locs, 'tcp.srcport']\n",
    "    df.loc[locs, 'udp.dstport'] = df.loc[locs, 'tcp.dstport']\n",
    "\n",
    "    # Delete TCP Columns\n",
    "    del df['tcp.srcport']\n",
    "    del df['tcp.dstport']\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Mask SRC and DST Ports larger than 1024 with value 66000\n",
    "def port_masking(df):\n",
    "    # Assign Masked SRC and DST Port Values in New Column\n",
    "    df['Masked Source Port'] = 66000\n",
    "    df['Masked Destination Port'] = 66000\n",
    "\n",
    "    # Mask SRC Ports\n",
    "    mask = df['udp.srcport'] < 1025\n",
    "    df.loc[mask, 'Masked Source Port'] = df.loc[mask, 'udp.srcport']\n",
    "\n",
    "    # Mask DST Ports\n",
    "    mask = df['udp.dstport'] < 1025\n",
    "    df.loc[mask, 'Masked Destination Port'] = df.loc[mask, 'udp.dstport']\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add Number of Packets to the DataFrame\n",
    "def add_num_packets(df):\n",
    "    # Retrieve Number of Packets\n",
    "    num_packets = len(df)\n",
    "\n",
    "    # Add Column \n",
    "    df['Number of Packets'] = num_packets\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add Trace Duration Info to the DataFrame\n",
    "def add_duration_time(df):\n",
    "    # Compute Duration\n",
    "    duration = df.iloc[len(df)-1, df.columns.get_loc('frame.time_epoch')] - df.iloc[0, df.columns.get_loc('frame.time_epoch')]\n",
    "\n",
    "    # Add Column\n",
    "    df['Trace Duration [s]'] = duration\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "    \n",
    "\n",
    "# Rename the DataFrame \n",
    "def rename_dataframe(df):\n",
    "    # Rename Columns\n",
    "    df.rename(columns = {'frame.number': 'No.', 'frame.time_epoch': 'Time Epoch', 'frame.time': 'Packet Arrival Time', 'ip.src': 'Source IP', 'ip.dst': 'Destination                           IP', '_ws.col.Protocol': 'Protocol', 'frame.len': 'Traffic Size [Byte]', 'udp.srcport': 'Source Port', 'udp.dstport': 'Destination Port'},                           inplace = True)\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Rearrange DataFrame Columns \n",
    "def rearrange_dataframe(df):\n",
    "    # New Columns\n",
    "    new_cols = ['No.', 'Time Epoch', 'Packet Arrival Time', 'Trace Duration [s]', 'Number of Packets', 'Source IP', 'Destination                           IP', 'Protocol', 'Traffic Size [Byte]', 'Source Port', 'Destination Port', 'Masked Source Port', 'Masked Destination Port']\n",
    "\n",
    "    # Rearrange\n",
    "    df = df[new_cols]\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Convert Pandas DataFrames to HDF5 \n",
    "def convert_df_to_HDF(df, csv_file, csv_dir, hdf_dir):\n",
    "    # File Directories \n",
    "    csv_dir = csv_dir + '/'\n",
    "\n",
    "    # File Naming\n",
    "    csv_name = csv_file.split(csv_dir)[1]\n",
    "    file_name = csv_name.split('.csv')[0]\n",
    "\n",
    "    # File Pathing\n",
    "    hdf_path = hdf_dir + file_name + ('.h5')\n",
    "\n",
    "    # Create HDF5 File\n",
    "    hdf = pd.HDFStore(hdf_path)\n",
    "\n",
    "    # Store Pandas Frame\n",
    "    hdf.put('PCAP', df)\n",
    "\n",
    "    # Close HDF5 File\n",
    "    hdf.close()\n",
    "\n",
    "\n",
    "# Rename CSV Files that Do Not Follow the Naming Convention of the Directory\n",
    "def rename_rogue_csv_files(csv_path):\n",
    "    # Get Rogue CSV Files\n",
    "    rogue_csv_files = []\n",
    "    _, _, filepath = next(os.walk(csv_path))\n",
    "\n",
    "    for file in filepath:\n",
    "        filename = csv_path + '/' + file\n",
    "        keyword = '/capture'\n",
    "        if keyword in filename:\n",
    "            rogue_csv_files.append(filename)\n",
    "\n",
    "    # Rename According to Epoch\n",
    "    counter = 0\n",
    "    for i in range(len(rogue_csv_files)):\n",
    "        df = pd.read_csv(rogue_csv_files[i])\n",
    "        epoch = int(df.loc[0, 'Time Epoch'])\n",
    "\n",
    "        old_path = rogue_csv_files[i]\n",
    "        old_name = old_path.split(csv_path + '/')[1]\n",
    "        new_path = csv_path + '/' + str(epoch) + '-' + old_name\n",
    "\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    print(\"Successfully renamed {} rogue files.\".format(counter))\n",
    "\n",
    "\n",
    "# Fix Broken PCAP Files\n",
    "def apply_pcap_fix(pcap_file_path: str, pcap_file: str):\n",
    "    # Naming\n",
    "    original_name = pcap_file.split(pcap_file_path + '/')[1]\n",
    "    fixed_name = 'fixed_' + original_name\n",
    "    pcap_file_destination = pcap_file_path + '/' + original_name\n",
    "\n",
    "    # Commands\n",
    "    command1 = 'cd /home/djuhera/pcapfix-1.1.4'\n",
    "    command2 = ('/home/djuhera/pcapfix-1.1.4/pcapfix -d {}').format(pcap_file)\n",
    "    command3 = ('mv /home/djuhera/notebooks/{} {}').format(fixed_name, pcap_file_destination)\n",
    "\n",
    "    # Execution\n",
    "    subprocess.check_call(command1, shell=True)\n",
    "    subprocess.run(command2, shell=True)\n",
    "    subprocess.check_call(command3, shell=True)\n",
    "\n",
    "\n",
    "# Remove Fraudulent PCAPs from List\n",
    "def delete_fraudulent_pcaps(pcap_list, fraudulent_pcaps):\n",
    "    for i in range(len(fraudulent_pcaps)):\n",
    "        file_name = fraudulent_pcaps[i]\n",
    "        index = pcap_list.index(file_name)\n",
    "        del pcap_list[index]\n",
    "\n",
    "    return pcap_list\n",
    "\n",
    "\n",
    "# Convert PCAP Files to CSV Frames and Optionally Store as HDF5\n",
    "def convert_pcap_to_csvframe(pcap_dir: str, pcap_file: str, csv_dir: str, store_HDF5: bool):\n",
    "    # PCAP File Location\n",
    "\n",
    "    # Replace File Suffix .pcap with .csv\n",
    "    pcap_csv_rename = []\n",
    "    for files in pcap_file:\n",
    "        pcap_csv_rename.append(files.replace('pcap', 'csv'))\n",
    "\n",
    "    # Generate List with Correct Pathing and CSV Name\n",
    "    csv_file = []\n",
    "    for files in pcap_csv_rename:\n",
    "        csv_file.append(files.replace(pcap_dir, csv_dir))\n",
    "\n",
    "    # TSHARK Commands\n",
    "    broken_pcaps = 0\n",
    "    for i in range(len(csv_file)):\n",
    "        try:\n",
    "            command = ('tshark -r {} -T fields '\n",
    "                    '-e frame.number '\n",
    "                    '-e frame.time_epoch '\n",
    "                    '-e frame.time '\n",
    "                    '-e ip.src '\n",
    "                    '-e ipv6.src '\n",
    "                    '-e ip.dst '\n",
    "                    '-e ipv6.dst '\n",
    "                    '-e _ws.col.Protocol '\n",
    "                    '-e frame.len '\n",
    "                    '-e tcp.srcport '\n",
    "                    '-e tcp.dstport '\n",
    "                    '-e udp.srcport '\n",
    "                    '-e udp.dstport '\n",
    "                    '-E header=y -E separator=, -E quote=d > {}').format(\n",
    "                pcap_file[i],\n",
    "                csv_file[i]\n",
    "            )\n",
    "            subprocess.check_call(command, shell=True)\n",
    "        except: \n",
    "            print(\"Found Broken PCAP at index {}\".format(i))\n",
    "            broken_pcaps = broken_pcaps + 1\n",
    "            apply_pcap_fix(pcap_dir, pcap_file[i])\n",
    "            command = ('tshark -r {} -T fields '\n",
    "                    '-e frame.number '\n",
    "                    '-e frame.time_epoch '\n",
    "                    '-e frame.time '\n",
    "                    '-e ip.src '\n",
    "                    '-e ipv6.src '\n",
    "                    '-e ip.dst '\n",
    "                    '-e ipv6.dst '\n",
    "                    '-e _ws.col.Protocol '\n",
    "                    '-e frame.len '\n",
    "                    '-e tcp.srcport '\n",
    "                    '-e tcp.dstport '\n",
    "                    '-e udp.srcport '\n",
    "                    '-e udp.dstport '\n",
    "                    '-E header=y -E separator=, -E quote=d > {}').format(\n",
    "                pcap_file[i],\n",
    "                csv_file[i]\n",
    "            )\n",
    "            subprocess.check_call(command, shell=True)\n",
    "\n",
    "\n",
    "        # Read CSV in Pandas\n",
    "        pd_data = pd.read_csv(csv_file[i])\n",
    "\n",
    "        # Apply Modifications\n",
    "        correct_ipv6(pd_data)\n",
    "        correct_UDP_TCP_ports(pd_data)\n",
    "        port_masking(pd_data)\n",
    "        add_num_packets(pd_data)\n",
    "        add_duration_time(pd_data)\n",
    "        rename_dataframe(pd_data)\n",
    "\n",
    "        # Rearrange Columns\n",
    "        pd_data = rearrange_dataframe(pd_data)\n",
    "\n",
    "        # Convert Pandas Frame to CSV\n",
    "        pd_data.to_csv(csv_file[i], index=False)\n",
    "\n",
    "        # Convert Pandas Frame to HDF5 File\n",
    "        if store_HDF5:\n",
    "            hdf_dir = '/home/djuhera/DATA/HDF_files/'\n",
    "            convert_df_to_HDF(pd_data, csv_file[i], csv_dir, hdf_dir)\n",
    "        else:\n",
    "            None\n",
    "    \n",
    "    # Rogue File Renaming\n",
    "    rename_rogue_csv_files(csv_dir)\n",
    "\n",
    "    # Final Notice\n",
    "    print(\"Done converting {} files.\".format(len(pcap_file)))\n",
    "    print(\"Fixed {} Broken PCAPS\".format(broken_pcaps))\n",
    "    if store_HDF5:\n",
    "        print(\"Files were also stored as HDF5.\") \n",
    "    else:\n",
    "        print(\"Files were not stored as HDF5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of Trace:  2020-08-12 13:03:59.220206\nEnd of Trace:  2020-08-14 21:27:23.430398\nTotal Duration of Traces: \n2 Days 8 Hours 23 Minutes 24 Seconds\n"
     ]
    }
   ],
   "source": [
    "# 1) Preliminary Trace Data Info\n",
    "csv_dir = '/home/djuhera/DATA/CSV_converts'\n",
    "start_time, end_time, duration, start_epoch, end_epoch = get_time_info(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Analyzed Sequences:  1\n",
      "Analyzed Sequences:  2\n",
      "Analyzed Sequences:  3\n",
      "Analyzed Sequences:  4\n",
      "Analyzed Sequences:  5\n",
      "Analyzed Sequences:  6\n",
      "Analyzed Sequences:  7\n",
      "End Exceeded.\n",
      "Analyzed Sequences:  8\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 2) Generate Graph Iteratively \n",
    "\n",
    "# Empty Graph\n",
    "g = nx.MultiDiGraph()\n",
    "\n",
    "# Additional Info\n",
    "packets = []\n",
    "traffic = []\n",
    "\n",
    "end_time = datetime.datetime(2020, 8, 12, 17, 00, 00, 00)\n",
    "\n",
    "# Iteration\n",
    "counter = 0\n",
    "in_range = True\n",
    "while in_range:\n",
    "    delta = datetime.timedelta(hours = 0.5)                                           # Set to 5 later\n",
    "    start = datetime.datetime(2020, 8, 12, 13, 3, 59, 00) + counter * delta\n",
    "    end = start + delta\n",
    "\n",
    "    if start > end_time:\n",
    "        in_range = False\n",
    "        print(\"Done.\")\n",
    "        break\n",
    "    \n",
    "    if end > end_time:\n",
    "        print(\"End Exceeded.\")\n",
    "        end = end_time\n",
    "    else:\n",
    "        end = end\n",
    "\n",
    "    time_window = [start, end]\n",
    "\n",
    "    # Apply Time Slicing\n",
    "    df = time_windowing2(csv_dir, time_window)\n",
    "\n",
    "    # Remove Protocols below Layer 4 (Transport Layer)\n",
    "    with suppress_stdout():\n",
    "        filter_protocols(df)\n",
    "\n",
    "    # Modify DataFrame\n",
    "    df.drop(['Time Epoch', 'Packet Arrival Time', 'Trace Duration [s]', 'Number of Packets', 'Masked Source Port', 'Masked Destination Port', 'Protocol', 'No.'], axis=1, inplace=True)\n",
    "    df['Packet Count'] = 1\n",
    "    df = df.groupby(['Source IP', 'Destination                           IP', 'Source Port', 'Destination Port']).sum()\n",
    "\n",
    "    # Add Edges to Graph\n",
    "    for index, row in df.iterrows():\n",
    "        g.add_edge(index[0], index[1], traffic_size=row['Traffic Size [Byte]'], packet_count=row['Packet Count'])\n",
    "\n",
    "    # Additional Info\n",
    "    traffic.append(df['Traffic Size [Byte]'].sum())\n",
    "    packets.append(df['Packet Count'].sum())\n",
    "    \n",
    "    # Delete DataFrame\n",
    "    del df\n",
    "\n",
    "    # Increment Counter\n",
    "    counter += 1\n",
    "\n",
    "    # Print\n",
    "    print(\"Analyzed Sequences: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes:  2934\nEdges:  223775\nComponents:  9\nDensity:  0.026003954251168623\nTotal Traffic [Byte]:  8767437922\nExchanged Packets:  23923913\n"
     ]
    }
   ],
   "source": [
    "num_nodes = g.number_of_nodes()\n",
    "num_edges = g.number_of_edges()\n",
    "num_components = nx.number_weakly_connected_components(g)\n",
    "graph_density = nx.density(g)\n",
    "\n",
    "total_traffic = sum(traffic)\n",
    "total_packets = sum(packets)\n",
    "\n",
    "print(\"Nodes: \", num_nodes)\n",
    "print(\"Edges: \", num_edges)\n",
    "print(\"Components: \", num_components)\n",
    "print(\"Density: \", graph_density)\n",
    "\n",
    "print(\"Total Traffic [Byte]: \", total_traffic)\n",
    "print(\"Exchanged Packets: \", total_packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "dir = '/home/djuhera/notebooks/plots/graph.json'\n",
    "save_graph_json(dir, g)\n",
    "\n",
    "# Import\n",
    "g_import = import_graph_json(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes:  2934\nEdges:  223775\nComponents:  9\nDensity:  0.026003954251168623\nTotal Traffic [Byte]:  8767437922\nExchanged Packets:  23923913\n"
     ]
    }
   ],
   "source": [
    "num_nodes = g_import.number_of_nodes()\n",
    "num_edges = g_import.number_of_edges()\n",
    "num_components = nx.number_weakly_connected_components(g_import)\n",
    "graph_density = nx.density(g_import)\n",
    "\n",
    "total_traffic = sum(traffic)\n",
    "total_packets = sum(packets)\n",
    "\n",
    "print(\"Nodes: \", num_nodes)\n",
    "print(\"Edges: \", num_edges)\n",
    "print(\"Components: \", num_components)\n",
    "print(\"Density: \", graph_density)\n",
    "\n",
    "print(\"Total Traffic [Byte]: \", total_traffic)\n",
    "print(\"Exchanged Packets: \", total_packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "graph_dir = '/home/djuhera/DATA/Graphs/traces_graph.json'\n",
    "G = import_graph_json(graph_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'networkx.classes.multidigraph.MultiDiGraph'>\n"
     ]
    }
   ],
   "source": [
    "print(type(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes:  13651\nEdges:  4689940\nComponents:  5\nDensity:  0.025169243863844993\n"
     ]
    }
   ],
   "source": [
    "# General Info\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "num_components = nx.number_weakly_connected_components(G)\n",
    "graph_density = nx.density(G)\n",
    "\n",
    "print(\"Nodes: \", num_nodes)\n",
    "print(\"Edges: \", num_edges)\n",
    "print(\"Components: \", num_components)\n",
    "print(\"Density: \", graph_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IP: 10.152.4.139 with Degree 179387\n\nIP: 10.152.4.51 with Degree 215281\n\nIP: 2001:4ca0:2203:2:907c:33ff:fec4:e45e with Degree 218583\n\nIP: 10.152.4.92 with Degree 249320\n\nIP: 10.152.4.151 with Degree 253411\n\nIP: 2001:4ca0:2203:0:921b:eff:fe0d:aa5 with Degree 336050\n\nIP: 2001:4ca0:2203:0:219:99ff:fe8a:757 with Degree 338140\n\nIP: 10.152.4.31 with Degree 436899\n\nIP: 10.152.4.15 with Degree 612293\n\nIP: 10.156.33.53 with Degree 1398465\n\n"
     ]
    }
   ],
   "source": [
    "# Get Extreme Nodes\n",
    "def get_max_degrees(degree_sequences, node_sequences, N):\n",
    "    # Lists to Fill\n",
    "    max_indices = []\n",
    "    max_nodes = []\n",
    "\n",
    "    # Get Max Degrees\n",
    "    sorted_deg_sequence = sorted(degree_sequences)\n",
    "    max_degrees = sorted_deg_sequence[-N:]\n",
    "\n",
    "    # GET Max Indices\n",
    "    for i in range(N):\n",
    "        index = degree_sequences.index(max_degrees[i])\n",
    "        max_indices.append(index)\n",
    "\n",
    "    # Get Max Nodes\n",
    "    for i in range(10):\n",
    "        node = node_sequences[max_indices[i]]\n",
    "        max_nodes.append(node)\n",
    "\n",
    "    # Print\n",
    "    output = []\n",
    "    for i in range(len(max_degrees)):\n",
    "        str = \"IP: {} with Degree {}\".format(max_nodes[i], max_degrees[i])\n",
    "        output.append(str)\n",
    "        print(str + \"\\n\")\n",
    "\n",
    "    # Return \n",
    "    return max_degrees, max_nodes \n",
    "\n",
    "\n",
    "# Sequences\n",
    "degree_sequences = [d for n, d in G.degree()]\n",
    "node_sequences = [n for n, d in G.degree()]\n",
    "\n",
    "# Get Info\n",
    "max_degrees, max_nodes = get_max_degrees(degree_sequences, node_sequences, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 1)\t9\n  (2, 3)\t39\n  (3, 2)\t39\n  (3, 24)\t2224\n  (3, 206)\t4\n  (3, 207)\t2\n  (3, 314)\t12\n  (3, 315)\t23\n  (3, 409)\t282\n  (3, 435)\t653\n  (3, 436)\t20\n  (3, 8055)\t16\n  (3, 10258)\t16\n  (4, 5)\t2\n  (5, 1)\t15\n  (5, 4)\t2\n  (5, 19)\t2\n  (5, 22)\t35\n  (5, 24)\t8885\n  (5, 32)\t14\n  (5, 84)\t7\n  (5, 85)\t7\n  (5, 89)\t7\n  (5, 90)\t21\n  (5, 91)\t7\n  :\t:\n  (13626, 1188)\t10\n  (13627, 1188)\t1\n  (13628, 1188)\t1\n  (13629, 1188)\t1\n  (13630, 1188)\t1\n  (13631, 1188)\t2\n  (13632, 1188)\t1\n  (13633, 1188)\t1\n  (13634, 1188)\t5\n  (13635, 1188)\t1\n  (13636, 1188)\t1\n  (13637, 1188)\t1\n  (13638, 1188)\t1\n  (13639, 1188)\t1\n  (13640, 1188)\t1\n  (13641, 1188)\t1\n  (13642, 1278)\t2\n  (13643, 16)\t1\n  (13644, 11365)\t1\n  (13645, 11358)\t2\n  (13646, 2670)\t1\n  (13647, 2074)\t2\n  (13648, 2074)\t1\n  (13649, 2074)\t1\n  (13650, 2098)\t1\n"
     ]
    }
   ],
   "source": [
    "# Adjecency Matrix\n",
    "A = nx.adjacency_matrix(G)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "[[0. 9. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# JNP Array\n",
    "A_jnp = jnp.array(nx.to_numpy_matrix(G))\n",
    "print(A_jnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Adjacency List\n",
    "\n",
    "# Get All Nodes and Edges\n",
    "nodes = list(G.nodes)\n",
    "edges = G.edges()\n",
    "\n",
    "# Define Dictionary\n",
    "d = {v: i for i, v in enumerate(nodes)}\n",
    "\n",
    "# Create Adjacency List [Without Edge Weights]\n",
    "adj_list = [(d[u], d[v]) for u, v in edges]\n",
    "\n",
    "print(adj_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}