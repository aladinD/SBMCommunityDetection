{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from typing import List\n",
    "import datetime\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager\n",
    "import socket\n",
    "from shutil import copyfile\n",
    "from contextlib import contextmanager\n",
    "import matplotlib.ticker as ticker\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import collections\n",
    "import json\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils ###\n",
    "\n",
    "# Suppress Console Output\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "# Encoder For TypeCasting Numpy Formats in DataFrame\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# Save Graph in JSON Format\n",
    "def save_graph_json(path, graph):\n",
    "    data = json_graph.node_link_data(graph)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, cls=MyEncoder)\n",
    "\n",
    "\n",
    "# Import Graph from JSON Format\n",
    "def import_graph_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    contents = json.loads(data)\n",
    "    return json_graph.node_link_graph(contents)\n",
    "\n",
    "\n",
    "# Get Extreme Nodes\n",
    "def get_max_degrees(degree_sequences, node_sequences, N):\n",
    "    # Lists to Fill\n",
    "    max_indices = []\n",
    "    max_nodes = []\n",
    "\n",
    "    # Get Max Degrees\n",
    "    sorted_deg_sequence = sorted(degree_sequences)\n",
    "    max_degrees = sorted_deg_sequence[-N:]\n",
    "\n",
    "    # GET Max Indices\n",
    "    for i in range(N):\n",
    "        index = degree_sequences.index(max_degrees[i])\n",
    "        max_indices.append(index)\n",
    "\n",
    "    # Get Max Nodes\n",
    "    for i in range(10):\n",
    "        node = node_sequences[max_indices[i]]\n",
    "        max_nodes.append(node)\n",
    "\n",
    "    # Print\n",
    "    output = []\n",
    "    for i in range(len(max_degrees)):\n",
    "        str = \"IP: {} with Degree {}\".format(max_nodes[i], max_degrees[i])\n",
    "        output.append(str)\n",
    "        print(str + \"\\n\")\n",
    "\n",
    "    # Return \n",
    "    return max_degrees, max_nodes \n",
    "\n",
    "\n",
    "# Get Time Sorted CSV List\n",
    "def get_csv_list(csv_dir):\n",
    "    # Capture Files in Time Sorted List \n",
    "    csv_files = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(csv_dir, \"*.csv\"))):\n",
    "        csv_files.append(file)\n",
    "\n",
    "    # Return \n",
    "    return csv_files\n",
    "\n",
    "\n",
    "# Remove Unwanted Protocols from DataFrame\n",
    "def filter_protocols(df):\n",
    "    # Dump Protocols\n",
    "    dump = ['IrDA', 'USB', 'DSL', 'ISDN', 'ITU', 'ARINC', 'Ethernet', 'Bluetooth', 'ARCnet', 'ARP', 'ATM', 'CHAP', 'CDP', 'DCAP', 'DTP', 'Econet', 'FDDI', 'ITU-T', 'HDLC', 'IEEE 802.11', 'IEEE 802.16', 'LACP', 'LattisNet', 'LocalTalk', 'L2F', 'L2TP', 'LLDP', 'MAC', 'Q.710', 'NDP', 'PAgP', 'PPP', 'PPTP', 'PAP', 'RPR', 'SLIP', 'StarLAN', 'STP', 'Token Ring', 'VTP', 'VEN', 'VLAN', 'ATM', 'IS-IS', 'SPB', 'MTP', 'NSP', 'ARP', 'MPLS', 'PPPoE', 'TIPC', 'CLNP', 'IPX', 'NAT', 'Routed-SMLT', 'SCCP', 'HSRP', 'VRRP', 'IP', 'IPv4', 'IPv6', 'ICMP', 'ARP', 'RIP', 'OSPF', 'IPSEC', 'AppleTalk', 'DECnet', 'IPX', 'SPX', 'IGMP', 'IPsec']\n",
    "\n",
    "    # Mask Entries and Remove Unwanted Protocols From DataFrame\n",
    "    mask = df['Protocol'].apply(lambda x: any(item for item in dump if item in x))\n",
    "    df.drop(df[mask].index, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract Source Services by Port\n",
    "def src_services(df):\n",
    "     counter = 1\n",
    "     for port in df['Masked Source Port'].unique():\n",
    "         if int(port) == 66000:\n",
    "             continue\n",
    "         else:\n",
    "             try:\n",
    "                 print(\"{} {}\".format(int(port), socket.getservbyport(int(port))))\n",
    "                 counter += 1\n",
    "             except:\n",
    "                 continue\n",
    "\n",
    "\n",
    "# Extract Destination Services by Port\n",
    "def dst_services(df):\n",
    "    counter = 1\n",
    "    for port in df['Masked Destination Port'].unique():\n",
    "         if int(port) == 66000:\n",
    "             continue\n",
    "         else:\n",
    "             try:\n",
    "                 print(\"{} {}\".format(int(port), socket.getservbyport(int(port))))\n",
    "                 counter += 1\n",
    "             except:\n",
    "                 continue \n",
    "\n",
    "\n",
    "# Extract Traffic Info by Service\n",
    "def traffic_by_service(df):\n",
    "    counter = 1\n",
    "    i = 0\n",
    "    for port in df.loc[:, 'Destination Port']:\n",
    "        try:\n",
    "            df.loc[i, 'Destination Port'] = socket.getservbyport(int(port))\n",
    "            counter += 1\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "    \n",
    "    return df.loc[:, 'Destination Port']\n",
    "\n",
    "\n",
    "# Get Complete Data Set Time Info\n",
    "def get_time_info(csv_dir):\n",
    "    # Locate CSV Files\n",
    "    csv_files = get_csv_list(csv_dir)\n",
    "\n",
    "    # Extract First and Last Element\n",
    "    df_start = pd.read_csv(csv_files[0])\n",
    "    df_end = pd.read_csv(csv_files[-1])\n",
    "\n",
    "    # Extract Start and End Time Info\n",
    "    start_epoch = df_start.loc[0, \"Time Epoch\"]\n",
    "    end_epoch = df_end.loc[len(df_end)-1, \"Time Epoch\"]\n",
    "\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_time = reference + datetime.timedelta(0, start_epoch)\n",
    "    end_time = reference + datetime.timedelta(0, end_epoch)\n",
    "\n",
    "    # Extract Trace Duration\n",
    "    duration = end_epoch - start_epoch\n",
    "    d = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=int(duration))\n",
    "\n",
    "    # Print\n",
    "    print(\"Start of Trace: \", start_time)\n",
    "    print(\"End of Trace: \", end_time)\n",
    "    print(\"Total Duration of Traces: \")\n",
    "    print(\"{} Days {} Hours {} Minutes {} Seconds\".format(d.day-1, d.hour, d.minute, d.second))\n",
    "\n",
    "    # Return\n",
    "    return start_time, end_time, duration, start_epoch, end_epoch\n",
    "\n",
    "\n",
    "# Get Complete Trace Time Info\n",
    "def get_time_info_old(df):\n",
    "    # Extract Start and End Epochs\n",
    "    start_epoch = df.loc[0, \"Time Epoch\"]\n",
    "    end_epoch = df.loc[len(df)-1, \"Time Epoch\"]\n",
    "\n",
    "    # Get Start and End Date\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_date = reference + datetime.timedelta(0, start_epoch)\n",
    "    end_date = reference + datetime.timedelta(0, end_epoch)\n",
    "\n",
    "    # Get Trace Duration\n",
    "    duration = end_epoch - start_epoch\n",
    "    d = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=int(duration))\n",
    "\n",
    "    # Print\n",
    "    print(\"Start of Trace: \", start_date)\n",
    "    print(\"End of Trace: \", end_date)\n",
    "    print(\"Total Duration of Traces: \")\n",
    "    print(\"{} Days {} Hours {} Minutes {} Seconds\".format(d.day-1, d.hour, d.minute, d.second))\n",
    "\n",
    "    # Return\n",
    "    return start_date, end_date, duration, start_epoch, end_epoch\n",
    "\n",
    "\n",
    "# Get Time Epoch Info from DateTime Object\n",
    "def get_epoch_info(start, end):\n",
    "    # Get Start and End Epoch\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_epoch = (start - reference).total_seconds()\n",
    "    end_epoch = (end - reference).total_seconds()\n",
    "\n",
    "    # Get Duration\n",
    "    duration = end_epoch - start_epoch \n",
    "\n",
    "    # Return \n",
    "    return start_epoch, end_epoch, duration\n",
    "\n",
    "\n",
    "# Get DateTime Info from Time Epoch\n",
    "def get_DateTime_from_epochs(start, end):\n",
    "    # Convert Epochs into DateTime Objects\n",
    "    reference = datetime.datetime(1970, 1, 1)\n",
    "    start_date = reference + datetime.timedelta(seconds = start)\n",
    "    end_date = reference + datetime.timedelta(seconds = end)\n",
    "\n",
    "    # Return\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "# Load all CSV Files in a single DataFrame\n",
    "def load_all(csv_dir):\n",
    "    # Get All CSV Files\n",
    "    csv_files = get_csv_list(csv_dir)\n",
    "\n",
    "    # Load into Pandas and Concatenate\n",
    "    df = pd.concat((pd.read_csv(file) for file in csv_files), axis=0, ignore_index = True)\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Window Slicing Function\n",
    "def time_windowing(csv_path: str, time_window: List):\n",
    "    # Load Data\n",
    "    df = load_all(csv_path)\n",
    "\n",
    "    # Get Time Info\n",
    "    with suppress_stdout():\n",
    "        start_date, end_date, trace_duration, trace_start_epoch, trace_end_epoch = get_time_info(df)\n",
    "\n",
    "    # Check if Time Window is Valid\n",
    "    window_start = time_window[0]\n",
    "    window_end = time_window[-1]\n",
    "\n",
    "    if window_start < start_date:\n",
    "        window_start = start_date\n",
    "        print(\"Adjusted Start of Time Window to: \", window_start)\n",
    "    if window_end > end_date:\n",
    "        window_end = end_date\n",
    "        print(\"Adjusted End of Time Window to: \", window_end)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    # Convert Window Start and Window End into Valid Time Epochs\n",
    "    window_start_epoch, window_end_epoch, window_duration = get_epoch_info(window_start, window_end)\n",
    "\n",
    "    # Find Indices in DataFrame that correspond to window_start_epoch and window_end_epoch by finding closest Time Epoch values\n",
    "    all_epochs = list(df['Time Epoch'].to_numpy())\n",
    "    closest_start_epoch = min(all_epochs, key=lambda x:abs(x-window_start_epoch))\n",
    "    closest_end_epoch = min(all_epochs, key=lambda x:abs(x-window_end_epoch))\n",
    "\n",
    "    loc_start = df.loc[df['Time Epoch'] == closest_start_epoch].index[0]\n",
    "    loc_end = df.loc[df['Time Epoch'] == closest_end_epoch].index[0]\n",
    "\n",
    "    locs = np.arange(loc_start, loc_end + 1)\n",
    "\n",
    "    # Construct DataFrame of Interest with given loc_start and loc_end\n",
    "    df_new = df.loc[locs]\n",
    "    df_new = df_new.reset_index()\n",
    "    del df_new['index']\n",
    "\n",
    "    # Return \n",
    "    return df_new\n",
    "\n",
    "\n",
    "# Window Slicing Function without Loading every CSV [Faster Performance]\n",
    "def time_windowing2(csv_path: str, time_window: List):\n",
    "    # Get Epochs List from CSV Path\n",
    "    csv_epochs = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(csv_path, \"*.csv\"))):\n",
    "        full_file_path = file\n",
    "        file_only = full_file_path.split(csv_dir + '/')[1]\n",
    "        epoch_only = file_only.split('-')[0]\n",
    "        csv_epochs.append(int(epoch_only))\n",
    "\n",
    "    # Get Time Info from Epochs\n",
    "    start_epoch = int(csv_epochs[0])\n",
    "    end_epoch = int(csv_epochs[-1])\n",
    "    start_date, end_date = get_DateTime_from_epochs(start_epoch, end_epoch)\n",
    "\n",
    "    # Convert Window Start and Window End into Valid Time Epochs\n",
    "    window_start = time_window[0]\n",
    "    window_end = time_window[-1]\n",
    "    window_start_epoch, window_end_epoch, window_duration = get_epoch_info(window_start, window_end)\n",
    "\n",
    "    # Check if Time Window is Valid by Converting Epochs into Integers and Comparing them with the CSV Names\n",
    "    window_start_epoch = int(window_start_epoch)\n",
    "    window_end_epoch = int(window_end_epoch)\n",
    "\n",
    "    if window_start_epoch < start_epoch:\n",
    "        window_start_epoch = start_epoch\n",
    "        print(\"Adjusted Start of Time Window to: \", start_date)\n",
    "    if window_end_epoch > end_epoch:\n",
    "        window_end_epoch = end_epoch\n",
    "        print(\"Adjusted End of Time Window to: \", end_date)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    # Find Indices in csv_epochs that correspond to window_start_epoch and window_end_epoch by finding closest Time Epoch values\n",
    "    closest_start_epoch = min(csv_epochs, key=lambda x:abs(x-window_start_epoch))\n",
    "    closest_end_epoch = min(csv_epochs, key=lambda x:abs(x-window_end_epoch))\n",
    "\n",
    "    loc_start = csv_epochs.index(closest_start_epoch)\n",
    "    loc_end = csv_epochs.index(closest_end_epoch)\n",
    "\n",
    "    # Construct DataFrame of Interest with given loc_start and loc_end\n",
    "    csv_files = get_csv_list(csv_path)\n",
    "    desired_csv_files = csv_files[loc_start:loc_end+1]\n",
    "\n",
    "    df = pd.concat((pd.read_csv(file) for file in desired_csv_files), axis=0, ignore_index = True)\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Get Time Sorted PCAP List\n",
    "def get_pcap_list(pcap_dir):\n",
    "    # Capture Files in Time Sorted List \n",
    "    pcap_files = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(pcap_dir, \"*.pcap\"))):\n",
    "        pcap_files.append(file)\n",
    "\n",
    "    # Return \n",
    "    return pcap_files\n",
    "\n",
    "\n",
    "# Merge IPv4 and IPv6 SRC and DST Columns \n",
    "def correct_ipv6(df):\n",
    "    # Locations with NaN Values\n",
    "    locs = df['ip.src'].isna()\n",
    "\n",
    "    # Replace Values\n",
    "    df.loc[locs, 'ip.src'] = df.loc[locs, 'ipv6.src']\n",
    "    df.loc[locs, 'ip.dst'] = df.loc[locs, 'ipv6.dst']\n",
    "\n",
    "    # Delete ipv6 columns\n",
    "    del df['ipv6.src']\n",
    "    del df['ipv6.dst']\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Merge UDP and TCP SRC and DST Ports\n",
    "def correct_UDP_TCP_ports(df):\n",
    "    # Locations with NaN Values\n",
    "    locs = df['udp.srcport'].isna()\n",
    "\n",
    "    # Replace Values\n",
    "    df.loc[locs, 'udp.srcport'] = df.loc[locs, 'tcp.srcport']\n",
    "    df.loc[locs, 'udp.dstport'] = df.loc[locs, 'tcp.dstport']\n",
    "\n",
    "    # Delete TCP Columns\n",
    "    del df['tcp.srcport']\n",
    "    del df['tcp.dstport']\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Mask SRC and DST Ports larger than 1024 with value 66000\n",
    "def port_masking(df):\n",
    "    # Assign Masked SRC and DST Port Values in New Column\n",
    "    df['Masked Source Port'] = 66000\n",
    "    df['Masked Destination Port'] = 66000\n",
    "\n",
    "    # Mask SRC Ports\n",
    "    mask = df['udp.srcport'] < 1025\n",
    "    df.loc[mask, 'Masked Source Port'] = df.loc[mask, 'udp.srcport']\n",
    "\n",
    "    # Mask DST Ports\n",
    "    mask = df['udp.dstport'] < 1025\n",
    "    df.loc[mask, 'Masked Destination Port'] = df.loc[mask, 'udp.dstport']\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add Number of Packets to the DataFrame\n",
    "def add_num_packets(df):\n",
    "    # Retrieve Number of Packets\n",
    "    num_packets = len(df)\n",
    "\n",
    "    # Add Column \n",
    "    df['Number of Packets'] = num_packets\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add Trace Duration Info to the DataFrame\n",
    "def add_duration_time(df):\n",
    "    # Compute Duration\n",
    "    duration = df.iloc[len(df)-1, df.columns.get_loc('frame.time_epoch')] - df.iloc[0, df.columns.get_loc('frame.time_epoch')]\n",
    "\n",
    "    # Add Column\n",
    "    df['Trace Duration [s]'] = duration\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "    \n",
    "\n",
    "# Rename the DataFrame \n",
    "def rename_dataframe(df):\n",
    "    # Rename Columns\n",
    "    df.rename(columns = {'frame.number': 'No.', 'frame.time_epoch': 'Time Epoch', 'frame.time': 'Packet Arrival Time', 'ip.src': 'Source IP', 'ip.dst': 'Destination                           IP', '_ws.col.Protocol': 'Protocol', 'frame.len': 'Traffic Size [Byte]', 'udp.srcport': 'Source Port', 'udp.dstport': 'Destination Port'},                           inplace = True)\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Rearrange DataFrame Columns \n",
    "def rearrange_dataframe(df):\n",
    "    # New Columns\n",
    "    new_cols = ['No.', 'Time Epoch', 'Packet Arrival Time', 'Trace Duration [s]', 'Number of Packets', 'Source IP', 'Destination                           IP', 'Protocol', 'Traffic Size [Byte]', 'Source Port', 'Destination Port', 'Masked Source Port', 'Masked Destination Port']\n",
    "\n",
    "    # Rearrange\n",
    "    df = df[new_cols]\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Convert Pandas DataFrames to HDF5 \n",
    "def convert_df_to_HDF(df, csv_file, csv_dir, hdf_dir):\n",
    "    # File Directories \n",
    "    csv_dir = csv_dir + '/'\n",
    "\n",
    "    # File Naming\n",
    "    csv_name = csv_file.split(csv_dir)[1]\n",
    "    file_name = csv_name.split('.csv')[0]\n",
    "\n",
    "    # File Pathing\n",
    "    hdf_path = hdf_dir + file_name + ('.h5')\n",
    "\n",
    "    # Create HDF5 File\n",
    "    hdf = pd.HDFStore(hdf_path)\n",
    "\n",
    "    # Store Pandas Frame\n",
    "    hdf.put('PCAP', df)\n",
    "\n",
    "    # Close HDF5 File\n",
    "    hdf.close()\n",
    "\n",
    "\n",
    "# Rename CSV Files that Do Not Follow the Naming Convention of the Directory\n",
    "def rename_rogue_csv_files(csv_path):\n",
    "    # Get Rogue CSV Files\n",
    "    rogue_csv_files = []\n",
    "    _, _, filepath = next(os.walk(csv_path))\n",
    "\n",
    "    for file in filepath:\n",
    "        filename = csv_path + '/' + file\n",
    "        keyword = '/capture'\n",
    "        if keyword in filename:\n",
    "            rogue_csv_files.append(filename)\n",
    "\n",
    "    # Rename According to Epoch\n",
    "    counter = 0\n",
    "    for i in range(len(rogue_csv_files)):\n",
    "        df = pd.read_csv(rogue_csv_files[i])\n",
    "        epoch = int(df.loc[0, 'Time Epoch'])\n",
    "\n",
    "        old_path = rogue_csv_files[i]\n",
    "        old_name = old_path.split(csv_path + '/')[1]\n",
    "        new_path = csv_path + '/' + str(epoch) + '-' + old_name\n",
    "\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    print(\"Successfully renamed {} rogue files.\".format(counter))\n",
    "\n",
    "\n",
    "# Fix Broken PCAP Files\n",
    "def apply_pcap_fix(pcap_file_path: str, pcap_file: str):\n",
    "    # Naming\n",
    "    original_name = pcap_file.split(pcap_file_path + '/')[1]\n",
    "    fixed_name = 'fixed_' + original_name\n",
    "    pcap_file_destination = pcap_file_path + '/' + original_name\n",
    "\n",
    "    # Commands\n",
    "    command1 = 'cd /home/djuhera/pcapfix-1.1.4'\n",
    "    command2 = ('/home/djuhera/pcapfix-1.1.4/pcapfix -d {}').format(pcap_file)\n",
    "    command3 = ('mv /home/djuhera/notebooks/{} {}').format(fixed_name, pcap_file_destination)\n",
    "\n",
    "    # Execution\n",
    "    subprocess.check_call(command1, shell=True)\n",
    "    subprocess.run(command2, shell=True)\n",
    "    subprocess.check_call(command3, shell=True)\n",
    "\n",
    "\n",
    "# Remove Fraudulent PCAPs from List\n",
    "def delete_fraudulent_pcaps(pcap_list, fraudulent_pcaps):\n",
    "    for i in range(len(fraudulent_pcaps)):\n",
    "        file_name = fraudulent_pcaps[i]\n",
    "        index = pcap_list.index(file_name)\n",
    "        del pcap_list[index]\n",
    "\n",
    "    return pcap_list\n",
    "\n",
    "\n",
    "# Convert PCAP Files to CSV Frames and Optionally Store as HDF5\n",
    "def convert_pcap_to_csvframe(pcap_dir: str, pcap_file: str, csv_dir: str, store_HDF5: bool):\n",
    "    # PCAP File Location\n",
    "\n",
    "    # Replace File Suffix .pcap with .csv\n",
    "    pcap_csv_rename = []\n",
    "    for files in pcap_file:\n",
    "        pcap_csv_rename.append(files.replace('pcap', 'csv'))\n",
    "\n",
    "    # Generate List with Correct Pathing and CSV Name\n",
    "    csv_file = []\n",
    "    for files in pcap_csv_rename:\n",
    "        csv_file.append(files.replace(pcap_dir, csv_dir))\n",
    "\n",
    "    # TSHARK Commands\n",
    "    broken_pcaps = 0\n",
    "    for i in range(len(csv_file)):\n",
    "        try:\n",
    "            command = ('tshark -r {} -T fields '\n",
    "                    '-e frame.number '\n",
    "                    '-e frame.time_epoch '\n",
    "                    '-e frame.time '\n",
    "                    '-e ip.src '\n",
    "                    '-e ipv6.src '\n",
    "                    '-e ip.dst '\n",
    "                    '-e ipv6.dst '\n",
    "                    '-e _ws.col.Protocol '\n",
    "                    '-e frame.len '\n",
    "                    '-e tcp.srcport '\n",
    "                    '-e tcp.dstport '\n",
    "                    '-e udp.srcport '\n",
    "                    '-e udp.dstport '\n",
    "                    '-E header=y -E separator=, -E quote=d > {}').format(\n",
    "                pcap_file[i],\n",
    "                csv_file[i]\n",
    "            )\n",
    "            subprocess.check_call(command, shell=True)\n",
    "        except: \n",
    "            print(\"Found Broken PCAP at index {}\".format(i))\n",
    "            broken_pcaps = broken_pcaps + 1\n",
    "            apply_pcap_fix(pcap_dir, pcap_file[i])\n",
    "            command = ('tshark -r {} -T fields '\n",
    "                    '-e frame.number '\n",
    "                    '-e frame.time_epoch '\n",
    "                    '-e frame.time '\n",
    "                    '-e ip.src '\n",
    "                    '-e ipv6.src '\n",
    "                    '-e ip.dst '\n",
    "                    '-e ipv6.dst '\n",
    "                    '-e _ws.col.Protocol '\n",
    "                    '-e frame.len '\n",
    "                    '-e tcp.srcport '\n",
    "                    '-e tcp.dstport '\n",
    "                    '-e udp.srcport '\n",
    "                    '-e udp.dstport '\n",
    "                    '-E header=y -E separator=, -E quote=d > {}').format(\n",
    "                pcap_file[i],\n",
    "                csv_file[i]\n",
    "            )\n",
    "            subprocess.check_call(command, shell=True)\n",
    "\n",
    "\n",
    "        # Read CSV in Pandas\n",
    "        pd_data = pd.read_csv(csv_file[i])\n",
    "\n",
    "        # Apply Modifications\n",
    "        correct_ipv6(pd_data)\n",
    "        correct_UDP_TCP_ports(pd_data)\n",
    "        port_masking(pd_data)\n",
    "        add_num_packets(pd_data)\n",
    "        add_duration_time(pd_data)\n",
    "        rename_dataframe(pd_data)\n",
    "\n",
    "        # Rearrange Columns\n",
    "        pd_data = rearrange_dataframe(pd_data)\n",
    "\n",
    "        # Convert Pandas Frame to CSV\n",
    "        pd_data.to_csv(csv_file[i], index=False)\n",
    "\n",
    "        # Convert Pandas Frame to HDF5 File\n",
    "        if store_HDF5:\n",
    "            hdf_dir = '/home/djuhera/DATA/HDF_files/'\n",
    "            convert_df_to_HDF(pd_data, csv_file[i], csv_dir, hdf_dir)\n",
    "        else:\n",
    "            None\n",
    "    \n",
    "    # Rogue File Renaming\n",
    "    rename_rogue_csv_files(csv_dir)\n",
    "\n",
    "    # Final Notice\n",
    "    print(\"Done converting {} files.\".format(len(pcap_file)))\n",
    "    print(\"Fixed {} Broken PCAPS\".format(broken_pcaps))\n",
    "    if store_HDF5:\n",
    "        print(\"Files were also stored as HDF5.\") \n",
    "    else:\n",
    "        print(\"Files were not stored as HDF5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General Traffic Data Analysis ###\n",
    "# To Do's: \n",
    "# * Nodes\n",
    "# * Edges\n",
    "# * Connected Components\n",
    "# * Graph Density\n",
    "# * Degree Distribution (CCDF)\n",
    "# * Max. Degree Analysis\n",
    "# * Accumulated Traffic Size \n",
    "# * Total Number of Sent Packets \n",
    "# * Dominant Protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of Trace:  2020-08-12 13:03:59.220206\nEnd of Trace:  2020-08-14 21:27:23.430398\nTotal Duration of Traces: \n2 Days 8 Hours 23 Minutes 24 Seconds\n"
     ]
    }
   ],
   "source": [
    "# 1) Preliminary Trace Data Info\n",
    "csv_dir = '/home/djuhera/DATA/CSV_converts'\n",
    "start_time, end_time, duration, start_epoch, end_epoch = get_time_info(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Collect Info from Graph\n",
    "\n",
    "# Import Graph \n",
    "dir = '/home/djuhera/DATA/Graphs/traces_graph.json'\n",
    "g = import_graph_json(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes:  13651\nEdges:  4689940\nComponents:  5\nDensity:  0.025169243863844993\n"
     ]
    }
   ],
   "source": [
    "# Get Nodes, Edges, Connected Components, Graph Density\n",
    "num_nodes = g.number_of_nodes()\n",
    "num_edges = g.number_of_edges()\n",
    "num_components = nx.number_weakly_connected_components(g)\n",
    "graph_density = nx.density(g)\n",
    "\n",
    "print(\"Nodes: \", num_nodes)\n",
    "print(\"Edges: \", num_edges)\n",
    "print(\"Components: \", num_components)\n",
    "print(\"Density: \", graph_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Investigate Degree Distribution\n",
    "\n",
    "# Degree Extraction\n",
    "degree_sequence = sorted([d for n, d in g.degree()], reverse=True)\n",
    "degreeCount = collections.Counter(degree_sequence)\n",
    "deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "# CDF and CCDF Computation\n",
    "cdf = np.cumsum(pd.Series(list(dict(nx.degree(g)).values())).value_counts(normalize=True).sort_index())\n",
    "ccdf = 1 - cdf\n",
    "\n",
    "# Plot\n",
    "mpl.use('pdf')\n",
    "plt.rc('font', family='serif') # , serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=10, titlesize=12)\n",
    "plt.rc('legend', fontsize=10)    \n",
    "\n",
    "width = 4.1\n",
    "height = width / 1.618\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# fig.subplots_adjust(left=.15, bottom=.16, right=.99, top=0.9)\n",
    "fig.set_size_inches(width, height)\n",
    "\n",
    "# cdf.plot(logx=True, logy=True, label=\"CDF\")\n",
    "ccdf.plot(logx=True, logy=True, label=\"CCDF\")\n",
    "\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "# ax.set_title(\"Node Degree Distribution [Log-Log-Scale]\")\n",
    "# ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig('/home/djuhera/notebooks/plots/node_ccdf.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IP: 10.152.4.139 with Degree 179387\n\nIP: 10.152.4.51 with Degree 215281\n\nIP: 2001:4ca0:2203:2:907c:33ff:fec4:e45e with Degree 218583\n\nIP: 10.152.4.92 with Degree 249320\n\nIP: 10.152.4.151 with Degree 253411\n\nIP: 2001:4ca0:2203:0:921b:eff:fe0d:aa5 with Degree 336050\n\nIP: 2001:4ca0:2203:0:219:99ff:fe8a:757 with Degree 338140\n\nIP: 10.152.4.31 with Degree 436899\n\nIP: 10.152.4.15 with Degree 612293\n\nIP: 10.156.33.53 with Degree 1398465\n\n"
     ]
    }
   ],
   "source": [
    "# 4) Investigate Max. Degrees\n",
    "\n",
    "# Sequences\n",
    "degree_sequences = [d for n, d in g.degree()]\n",
    "node_sequences = [n for n, d in g.degree()]\n",
    "\n",
    "# Get Info\n",
    "max_degrees, max_nodes = get_max_degrees(degree_sequences, node_sequences, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Analyzed Sequences:  1\n",
      "For End Time:  2020-08-12 17:03:59\n",
      "Analyzed Sequences:  2\n",
      "For End Time:  2020-08-12 21:03:59\n",
      "Analyzed Sequences:  3\n",
      "For End Time:  2020-08-13 01:03:59\n",
      "Analyzed Sequences:  4\n",
      "For End Time:  2020-08-13 05:03:59\n",
      "Analyzed Sequences:  5\n",
      "For End Time:  2020-08-13 09:03:59\n",
      "Analyzed Sequences:  6\n",
      "For End Time:  2020-08-13 13:03:59\n",
      "Analyzed Sequences:  7\n",
      "For End Time:  2020-08-13 17:03:59\n",
      "Analyzed Sequences:  8\n",
      "For End Time:  2020-08-13 21:03:59\n",
      "Analyzed Sequences:  9\n",
      "For End Time:  2020-08-14 01:03:59\n",
      "Analyzed Sequences:  10\n",
      "For End Time:  2020-08-14 05:03:59\n",
      "Analyzed Sequences:  11\n",
      "For End Time:  2020-08-14 09:03:59\n",
      "Analyzed Sequences:  12\n",
      "For End Time:  2020-08-14 13:03:59\n",
      "Analyzed Sequences:  13\n",
      "For End Time:  2020-08-14 17:03:59\n",
      "Analyzed Sequences:  14\n",
      "For End Time:  2020-08-14 21:03:59\n",
      "End Exceeded.\n",
      "Analyzed Sequences:  15\n",
      "For End Time:  2020-08-14 21:27:23.430398\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 5) Get Traffic and Sent Packets \n",
    "\n",
    "# Lists\n",
    "packets = []\n",
    "traffic = []\n",
    "dom_protocols = []\n",
    "dom_services = []\n",
    "\n",
    "# TESTING ONLY\n",
    "# end_time = datetime.datetime(2020, 8, 12, 17, 00, 00, 00)\n",
    "\n",
    "# Iteration\n",
    "counter = 0\n",
    "in_range = True\n",
    "while in_range:\n",
    "    delta = datetime.timedelta(hours = 4)                                           # Set to 5 later\n",
    "    start = datetime.datetime(2020, 8, 12, 13, 3, 59, 00) + counter * delta\n",
    "    end = start + delta\n",
    "\n",
    "    if start > end_time:\n",
    "        in_range = False\n",
    "        print(\"Done.\")\n",
    "        break\n",
    "    \n",
    "    if end > end_time:\n",
    "        print(\"End Exceeded.\")\n",
    "        end = end_time\n",
    "    else:\n",
    "        end = end\n",
    "\n",
    "    time_window = [start, end]\n",
    "\n",
    "    # Apply Time Slicing\n",
    "    df = time_windowing2(csv_dir, time_window)\n",
    "\n",
    "    # Remove Protocols below Layer 4 (Transport Layer)\n",
    "    with suppress_stdout():\n",
    "        filter_protocols(df)\n",
    "\n",
    "    # Get Dominant Protocol\n",
    "    by_protocol = df.loc[:, ['Protocol', 'Traffic Size [Byte]']].groupby('Protocol').aggregate('sum')\n",
    "\n",
    "    # Accumulate Protocols\n",
    "    firstTime_P = True\n",
    "    if firstTime_P:\n",
    "        a = by_protocol\n",
    "        firstTime_P = False\n",
    "    else: \n",
    "        a = pd.concat([a, by_protocol])\n",
    "\n",
    "    with suppress_stdout():\n",
    "        protocol_list = by_protocol.reset_index().values.tolist()\n",
    "\n",
    "    max_vals = []\n",
    "    for i in range(len(protocol_list)):\n",
    "        val = (protocol_list[i])[1]\n",
    "        max_vals.append(val)\n",
    "\n",
    "    sorted_max_vals = sorted(max_vals)\n",
    "    max_val = sorted_max_vals[-1]\n",
    "    max_val_index = max_vals.index(max_val)\n",
    "    dom_protocol = protocol_list[max_val_index]\n",
    "\n",
    "    dom_protocols.append(dom_protocol)\n",
    "\n",
    "    # Get Dominant Service\n",
    "    by_service = df.loc[df['Masked Destination Port'] != 66000, ['Destination Port', 'Traffic Size [Byte]']].groupby('Destination Port').sum()\n",
    "    by_service.reset_index(inplace=True)\n",
    "    by_service.loc[:, 'Destination Port'] = traffic_by_service(by_service)\n",
    "    by_service.columns = ['Service', 'Sum of Length']\n",
    "    by_service.set_index('Service', inplace=True)\n",
    "\n",
    "    # Accumulate Services\n",
    "    firstTime_S = True\n",
    "    if firstTime_S:\n",
    "        b = by_service\n",
    "        firstTime_S = False\n",
    "    else: \n",
    "        b = pd.concat([b, by_service])\n",
    "\n",
    "    with suppress_stdout():\n",
    "        services_list = by_service.reset_index().values.tolist()\n",
    "\n",
    "    max_vals = []\n",
    "    for i in range(len(services_list)):\n",
    "        val = (services_list[i])[1]\n",
    "        max_vals.append(val)\n",
    "\n",
    "    sorted_max_vals = sorted(max_vals)\n",
    "    max_val = sorted_max_vals[-1]\n",
    "    max_val_index = max_vals.index(max_val)\n",
    "    dom_service = services_list[max_val_index]\n",
    "\n",
    "    dom_services.append(dom_service)\n",
    "\n",
    "    # Modify DataFrame\n",
    "    df.drop(['Time Epoch', 'Packet Arrival Time', 'Trace Duration [s]', 'Number of Packets', 'Masked Source Port', 'Masked Destination Port', 'Protocol', 'No.'], axis=1, inplace=True)\n",
    "    df['Packet Count'] = 1\n",
    "    df = df.groupby(['Source IP', 'Destination                           IP', 'Source Port', 'Destination Port']).sum()\n",
    "\n",
    "    # Additional Info\n",
    "    traffic.append(df['Traffic Size [Byte]'].sum())\n",
    "    packets.append(df['Packet Count'].sum())\n",
    "    \n",
    "    # Delete DataFrame\n",
    "    del df\n",
    "\n",
    "    # Increment Counter\n",
    "    counter += 1\n",
    "\n",
    "    # Print\n",
    "    print(\"Analyzed Sequences: \", counter)\n",
    "    print(\"For End Time: \", end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Traffic [Byte]:  199614778555\nExchanged Packets:  573089330\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Traffic and Packets\n",
    "total_traffic = sum(traffic)\n",
    "total_packets = sum(packets)\n",
    "\n",
    "print(\"Total Traffic [Byte]: \", total_traffic)\n",
    "print(\"Exchanged Packets: \", total_packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Lists\n",
    "all = []\n",
    "all.append(packets)\n",
    "all.append(traffic)\n",
    "all.append(dom_protocols)\n",
    "all.append(dom_services)\n",
    "\n",
    "with open('/home/djuhera/notebooks/analysis_general.txt', 'w') as fp:\n",
    "    json.dump(all, fp, cls=MyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "with open('/home/djuhera/notebooks/analysis_general.txt', 'r') as fp:\n",
    "    all_loaded = json.load(fp)\n",
    "\n",
    "packets = all[0]\n",
    "traffic = all[1]\n",
    "dom_protocols = all[2]\n",
    "dom_services = all[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACCUMULATED:  [['TCP', 66530585711], ['UDP', 28533135955]]\n"
     ]
    }
   ],
   "source": [
    "# Analyze Dominant Protocols\n",
    "protocols = []\n",
    "for i in range(len(dom_protocols)):\n",
    "    name = (dom_protocols[i])[0]\n",
    "    protocols.append(name)\n",
    "\n",
    "# Unique Protocols\n",
    "unique_protocols = np.unique(np.array(protocols))\n",
    "\n",
    "# Accumulated Statistics\n",
    "accumulated_protocols = []\n",
    "for i in range(len(unique_protocols)):\n",
    "    key = unique_protocols[i]\n",
    "    indices = [i for i, x in enumerate(protocols) if x == key]\n",
    "    \n",
    "    connects = 0\n",
    "    for i in range(len(indices)):\n",
    "        connects += (dom_protocols[indices[i]])[1]\n",
    "\n",
    "    item = [key, connects]\n",
    "    accumulated_protocols.append(item)\n",
    "\n",
    "# print(\"DOM PROTOCOLS: \", dom_protocols)\n",
    "# print(\"PROTOCOLS: \", protocols)\n",
    "# print(\"UNIQUE PROTOCOLS: \", unique_protocols)\n",
    "print(\"ACCUMULATED: \", accumulated_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "mpl.use('pdf')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=10, titlesize=12)\n",
    "plt.rc('legend', fontsize=10)    \n",
    "\n",
    "width = 4.1\n",
    "height = width / 1.618\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# fig.subplots_adjust(left=0.0001, bottom=0.0001, right=0.002, top=0.9)\n",
    "fig.set_size_inches(width, height)\n",
    "\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "protocols = ['TCP', 'UDP']\n",
    "sum_connects = 66530585711 + 28533135955\n",
    "connects = [66530585711/total_traffic * 100, 28533135955/total_traffic * 100]\n",
    "ax.bar(protocols, connects)\n",
    "\n",
    "ax.set_xlabel(\"Protocols\")\n",
    "ax.set_ylabel(\"\\% of Traffic\")\n",
    "ax.set_title(\"Dominant Protocols by Traffic Allocation\")\n",
    "\n",
    "fig.savefig('/home/djuhera/notebooks/plots/dom_protocols.pdf', bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot All\n",
    "mpl.use('pdf')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=6)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=10, titlesize=12)\n",
    "plt.rc('legend', fontsize=10)    \n",
    "\n",
    "width = 4.1\n",
    "height = width / 1.618\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.subplots_adjust(left=0.0001, bottom=0.0001, right=0.002, top=0.9)\n",
    "fig.set_size_inches(width, height)\n",
    "\n",
    "# Adjust for Latex\n",
    "val = a\n",
    "idx = val.index\n",
    "new_idx = []\n",
    "for i in range(len(idx)):\n",
    "    word = idx[i]\n",
    "    new_word = word.replace('_', '\\_')\n",
    "    new_idx.append(new_word)\n",
    "\n",
    "val['Protocol'] = new_idx\n",
    "val = val.set_index('Protocol')\n",
    "\n",
    "ax = val.divide(np.sum(val['Traffic Size [Byte]'].values) / 100.).plot(kind='bar')\n",
    "ax.get_legend().remove()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "ax.set_xlabel(\"Protocols\")\n",
    "ax.set_ylabel(\"\\% of Traffic\")\n",
    "ax.set_title(\"Dominant Protocols by Traffic Allocation\")\n",
    "\n",
    "ax.figure.set_size_inches(width, height)\n",
    "\n",
    "plt.savefig('/home/djuhera/notebooks/plots/dom_protocols_detail.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              Traffic Size [Byte]       Protocol\nProtocol                                        \nBROWSER                     55756        BROWSER\nCLDAP                       24623          CLDAP\nDB-LSP-DISC                488144    DB-LSP-DISC\nDCERPC                      89509         DCERPC\nDHCPv6                      55572         DHCPv6\nDNS                       2941480            DNS\nDRSUAPI                     22400        DRSUAPI\nEPM                         38200            EPM\nHCrt                          308           HCrt\nHTTP                       425639           HTTP\nHTTP/XML                    18335       HTTP/XML\nKRB5                       560597           KRB5\nLDAP                      2919090           LDAP\nLLMNR                      126216          LLMNR\nMANOLITO                      389       MANOLITO\nMDNS                      2765624           MDNS\nMOUNT                         180          MOUNT\nNBNS                       119600           NBNS\nNBSS                        14275           NBSS\nNFS                         64676            NFS\nNLM                           416            NLM\nNTP                         19644            NTP\nOCSP                         1946           OCSP\nPKIX-CRL                     3145       PKIX-CRL\nPortmap                       600        Portmap\nRPC_NETLOGON                 8856  RPC\\_NETLOGON\nRTCP                          880           RTCP\nSMB                         29093            SMB\nSMB2                      7586145           SMB2\nSNMP                     39804094           SNMP\nSRVSVC                        596         SRVSVC\nSSDP                         1432           SSDP\nSSH                      39747335            SSH\nSSHv2                       14309          SSHv2\nSSL                       7338867            SSL\nSSLv2                       98180          SSLv2\nSSLv3                        1464          SSLv3\nTCP                     390651684            TCP\nTLSv1                     2385116          TLSv1\nTLSv1.2                  38383574        TLSv1.2\nTLSv1.3                   6607781        TLSv1.3\nTZSP                          274           TZSP\nUDP                     452635297            UDP\n---------------------------------------------------------------\n               Traffic Size [Byte]\nProtocol                          \nBROWSER                      55756\nCLDAP                        24623\nDB-LSP-DISC                 488144\nDCERPC                       89509\nDHCPv6                       55572\nDNS                        2941480\nDRSUAPI                      22400\nEPM                          38200\nHCrt                           308\nHTTP                        425639\nHTTP/XML                     18335\nKRB5                        560597\nLDAP                       2919090\nLLMNR                       126216\nMANOLITO                       389\nMDNS                       2765624\nMOUNT                          180\nNBNS                        119600\nNBSS                         14275\nNFS                          64676\nNLM                            416\nNTP                          19644\nOCSP                          1946\nPKIX-CRL                      3145\nPortmap                        600\nRPC\\_NETLOGON                 8856\nRTCP                           880\nSMB                          29093\nSMB2                       7586145\nSNMP                      39804094\nSRVSVC                         596\nSSDP                          1432\nSSH                       39747335\nSSHv2                        14309\nSSL                        7338867\nSSLv2                        98180\nSSLv3                         1464\nTCP                      390651684\nTLSv1                      2385116\nTLSv1.2                   38383574\nTLSv1.3                    6607781\nTZSP                           274\nUDP                      452635297\n"
     ]
    }
   ],
   "source": [
    "val = a\n",
    "idx = val.index\n",
    "print(val)\n",
    "\n",
    "new_idx = []\n",
    "for i in range(len(idx)):\n",
    "    word = idx[i]\n",
    "    new_word = word.replace('_', '\\_')\n",
    "    new_idx.append(new_word)\n",
    "\n",
    "# Add Column\n",
    "val['Protocol'] = new_idx\n",
    "\n",
    "# val = val.reindex(new_idx)\n",
    "val = val.set_index('Protocol')\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(val)\n",
    "\n",
    "# val.reindex(new_idx)\n",
    "# print(val.head())\n",
    "\n",
    "# print(idx)\n",
    "# print(new_idx)\n",
    "\n",
    "# print(a['Traffic Size [Byte]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACCUMULATED:  [['https', 4162085867], ['snmp', 586396434]]\n"
     ]
    }
   ],
   "source": [
    "# Analyze Dominant Services\n",
    "services = []\n",
    "for i in range(len(dom_services)):\n",
    "    name = (dom_services[i])[0]\n",
    "    services.append(name)\n",
    "\n",
    "# Unique Protocols\n",
    "unique_services = np.unique(np.array(services))\n",
    "\n",
    "# Accumulated Statistics\n",
    "accumulated_services = []\n",
    "for i in range(len(unique_services)):\n",
    "    key = unique_services[i]\n",
    "    indices = [i for i, x in enumerate(services) if x == key]\n",
    "    \n",
    "    connects = 0\n",
    "    for i in range(len(indices)):\n",
    "        connects += (dom_services[indices[i]])[1]\n",
    "\n",
    "    item = [key, connects]\n",
    "    accumulated_services.append(item)\n",
    "\n",
    "# print(\"SERVICES: \", services)\n",
    "# print(\"DOM Services: \", dom_services)\n",
    "# print(\"UNIQUE SERVICES: \", unique_services)\n",
    "print(\"ACCUMULATED: \", accumulated_services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_axes([0, 0, 1, 1])\n",
    "#services = ['https', 'snmp']\n",
    "#sum_connects = 4162085867 + 586396434\n",
    "#connects = [4162085867/sum_connects, 586396434/sum_connects]\n",
    "#ax.bar(services, connects)\n",
    "#plt.xlabel(\"Services\")\n",
    "#plt.ylabel(\"% of Connections\")\n",
    "#plt.title(\"Dominant Services\")\n",
    "#plt.show()\n",
    "\n",
    "# Plot\n",
    "mpl.use('pdf')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=10, titlesize=12)\n",
    "plt.rc('legend', fontsize=10)    \n",
    "\n",
    "width = 4.1\n",
    "height = width / 1.618\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# fig.subplots_adjust(left=0.0001, bottom=0.0001, right=0.002, top=0.9)\n",
    "fig.set_size_inches(width, height)\n",
    "\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "services = ['https', 'snmp']\n",
    "sum_connects = 4162085867 + 586396434\n",
    "connects = [4162085867/total_traffic * 100, 586396434/total_traffic * 100]\n",
    "ax.bar(services, connects)\n",
    "\n",
    "ax.set_xlabel(\"Services\")\n",
    "ax.set_ylabel(\"\\% of Traffic\")\n",
    "ax.set_title(\"Dominant Services by Traffic Allocation\")\n",
    "\n",
    "fig.savefig('/home/djuhera/notebooks/plots/dom_services.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot All\n",
    "mpl.use('pdf')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=10, titlesize=12)\n",
    "plt.rc('legend', fontsize=10)    \n",
    "\n",
    "width = 4.1\n",
    "height = width / 1.618\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.subplots_adjust(left=0.0001, bottom=0.0001, right=0.002, top=0.9)\n",
    "# fig.set_size_inches(width, height)\n",
    "\n",
    "val = b\n",
    "val.rename(columns={'Sum of Length': 'Traffic Size [Byte]'}, inplace=True)\n",
    "# ax = val.divide(np.sum(val['Sum of Length'].values) / 100.).plot(kind='bar')\n",
    "ax = val.divide(np.sum(val['Traffic Size [Byte]'].values) / 100.).plot(kind='bar')\n",
    "ax.get_legend().remove()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "ax.set_xlabel(\"Services\")\n",
    "ax.set_ylabel(\"\\% of Traffic\")\n",
    "ax.set_title(\"Dominant Services by Traffic Allocation\")\n",
    "\n",
    "ax.figure.set_size_inches(width, height)\n",
    "\n",
    "plt.savefig('/home/djuhera/notebooks/plots/dom_services_detail.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}