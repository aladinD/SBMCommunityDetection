{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU used:  True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/djuhera/es-sbm-master')\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import vis\n",
    "import degcor\n",
    "import overlapping\n",
    "import bernoulli\n",
    "import time\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = 'cpu'\n",
    "\n",
    "# Suppress Console Output\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            \n",
    "\n",
    "def make_utilities(population_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Make utility values to weight samples with.\n",
    "\n",
    "    Args:\n",
    "        population_size:\n",
    "\n",
    "    Returns:\n",
    "        utility_values: Shape (S,).\n",
    "    \"\"\"\n",
    "    utility_values = np.log(population_size / 2. + 1) - \\\n",
    "                     np.log(np.arange(1, population_size + 1))\n",
    "    utility_values[utility_values < 0] = 0.\n",
    "    utility_values /= np.sum(utility_values)\n",
    "    utility_values -= 1. / population_size\n",
    "    utility_values = utility_values.astype(np.float32)\n",
    "    return torch.tensor(utility_values)\n",
    "\n",
    "\n",
    "def scale_noise(noise: torch.Tensor, fitness: torch.Tensor, log_probs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Scale noise with the fitness values. Fitness values are assigned to noise\n",
    "    vectors based on the `log_probs` vector. The better the `log_probs` vector,\n",
    "    the higher the fitness and thus the more the gradient should point into\n",
    "    that direction.\n",
    "\n",
    "    Args:\n",
    "        noise: Noise used to perturb the parameters (S, |V|, k).\n",
    "        fitness: Vector of scalars, first element `fitness[0]` is the fitness corresponding\n",
    "            to the best sample, the last element `fitness[-1] is the fitness\n",
    "            for the worst sample. Has shape (S).\n",
    "        log_probs: Log prob resulting from the sampled placements. Has shape (S,).\n",
    "\n",
    "    Returns:\n",
    "        scaled_noise: Noise scaled with fitness values. Has shape (S, |V|, d).\n",
    "    \"\"\"\n",
    "    sorted_values, indices = torch.sort(-1. * log_probs)\n",
    "    scaled_noise = fitness.unsqueeze(1).unsqueeze(2) * noise[indices, :, :]\n",
    "    return scaled_noise\n",
    "\n",
    "\n",
    "def apply_gradient(params: torch.Tensor, grads: torch.Tensor, lr: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply the gradient to the parameters.\n",
    "\n",
    "    Args:\n",
    "        params: Tensor with parameters of shape (|V|, d).\n",
    "        grads: Tensor with gradients of shape (|V|, d).\n",
    "\n",
    "    Returns:\n",
    "        params_prime: Tensor of shape (|V|, d).\n",
    "    \"\"\"\n",
    "    params_prime = params + lr * grads\n",
    "    return params_prime\n",
    "\n",
    "\n",
    "def calculate_gradient(scaled_noise: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calcualte the gradient from the scaled noise.\n",
    "\n",
    "    Args:\n",
    "        scaled_noise: Tensor of shape (S, |V|, d).\n",
    "\n",
    "    Returns:\n",
    "        grad: Tensor of shape (|V|, d).\n",
    "    \"\"\"\n",
    "    grad = torch.sum(scaled_noise, 0)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sample_placements(mutated_params: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample indices from mutated paramters.\n",
    "\n",
    "    Args:\n",
    "        mutated_params: Shape (S, |V|, k)\n",
    "\n",
    "    Returns:\n",
    "        memberships: Shape (S, |V|).\n",
    "    \"\"\"\n",
    "    # cumsum = torch.cumsum(torch.softmax(mutated_params, dim=-1), dim=-1)\n",
    "    # noise = torch.rand(mutated_params.shape[0], mutated_params.shape[1], 1, requires_grad=False)\n",
    "    # indices = torch.argmin((cumsum < noise).int(), dim=-1)\n",
    "    indices = torch.distributions.Categorical(logits=mutated_params).sample()\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_propensities(mutated_params: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create propensities for parameters, i.e., soft assignment.\n",
    "\n",
    "    Args:\n",
    "        mutated_params: Shape (S, |V|, k)\n",
    "\n",
    "    Returns:\n",
    "        memberships: Shape (S, |V|, k).\n",
    "    \"\"\"\n",
    "    return torch.softmax(mutated_params, -1)\n",
    "\n",
    "\n",
    "def mutate_params(params: torch.Tensor, num_samples: int, scale: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create `num_samples` mutation of parameters.\n",
    "\n",
    "    Args:\n",
    "        params: Shape (|V|, k)\n",
    "\n",
    "    Returns:\n",
    "        mutated_params: Shape (S, |V|, k)\n",
    "    \"\"\"\n",
    "    noise = torch.randn([num_samples, params.shape[0], params.shape[1]], requires_grad=False)\n",
    "    mutated_params = torch.unsqueeze(params, 0) + noise\n",
    "    return noise, mutated_params\n",
    "\n",
    "\n",
    "def calc_scale_decrease(num_iter: int, fraction: float, target_scale: float) -> float:\n",
    "    stop = num_iter * fraction\n",
    "    m = (target_scale - 1.) / stop\n",
    "    return m\n",
    "\n",
    "\n",
    "def learned_embeddings(constants: Dict[str, torch.Tensor], population_size: int, k: int,\n",
    "                       num_nodes: int, driver: callable, make_membership: callable,\n",
    "                       lr: float, num_iter=10000, params: torch.Tensor=None,\n",
    "                       patience: int=None) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        constants: Dictionary containing constant arguments to driver functions.\n",
    "            For example, edges, in_degrees, out_degrees. Check the signature call\n",
    "            of a specific driver function to determine the fields of this dict.\n",
    "        population_size: Number of samples for ES.\n",
    "        k: Number of groups.\n",
    "        num_nodes: Number of nodes in graph.\n",
    "        driver: Callable that implements the calculation of the log-prob and\n",
    "            estimates parameters of a model.\n",
    "        lr: Learning rate that should be used.\n",
    "        num_iter: Number of iterations that should be executed.\n",
    "        params: Initial guess of parameters, if None start from uniform distribution\n",
    "            over groups.\n",
    "        patience: Extension of learning after new best log prob has been found.\n",
    "\n",
    "    Returns:\n",
    "        Dict\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        # Start from a uniform distribution over group memberships.\n",
    "        params = torch.ones(num_nodes, k, requires_grad=False, device=DEV)\n",
    "    fitness = make_utilities(population_size)\n",
    "\n",
    "    target_scale = 1e-3\n",
    "    fraction = 0.8\n",
    "    ascend = calc_scale_decrease(num_iter, fraction, target_scale)\n",
    "    all_losses = []\n",
    "    best_loss = 1e9\n",
    "    best_params = None\n",
    "    best_eta = None\n",
    "    best_assignment = None\n",
    "    patience = int(num_iter * 0.1) if patience is None else patience\n",
    "    iter = 0\n",
    "    while iter < num_iter:\n",
    "        scale = float(np.clip(iter * ascend + 1, target_scale, 1.))\n",
    "        # scale = 1.\n",
    "        iter += 1\n",
    "        noise, mutations = mutate_params(params, population_size, scale)\n",
    "        assignments = make_membership(mutations)\n",
    "        log_probs, estimates = driver(assignments, k, **constants)\n",
    "        all_losses.append(log_probs.cpu().detach().numpy())\n",
    "\n",
    "        bl = torch.min(-1. * log_probs)\n",
    "        if bl <= best_loss:\n",
    "            best_loss = bl.cpu().detach()\n",
    "            idx = torch.argmax(log_probs)\n",
    "            best_params = mutations[idx, :, :]\n",
    "            best_eta = estimates['etas'][idx]\n",
    "            best_assignment = assignments[idx]\n",
    "            if iter + patience > num_iter and bl < best_loss:\n",
    "                num_iter = iter + patience\n",
    "        if iter % 10 == 0:\n",
    "            print(iter, scale,  best_loss, torch.mean(log_probs))\n",
    "\n",
    "        scaled_noise = scale_noise(noise, fitness, log_probs)\n",
    "        grads = calculate_gradient(scaled_noise)\n",
    "        params = apply_gradient(params, grads, lr)\n",
    "\n",
    "    return {\n",
    "        \"best_eta\": best_eta.cpu(),\n",
    "        \"best_assignment\": best_assignment.cpu(),\n",
    "        \"best_params\": best_params.cpu(),\n",
    "        \"best_log_prob\": -1. * bl,\n",
    "        \"params\": params.cpu()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10 0.9839446428571429 tensor(55.8902) tensor(-61.2887)\n",
      "20 0.9661053571428572 tensor(55.1134) tensor(-56.2733)\n",
      "30 0.9482660714285714 tensor(55.0897) tensor(-56.1490)\n",
      "40 0.9304267857142857 tensor(55.0836) tensor(-56.1441)\n",
      "50 0.9125875 tensor(55.0822) tensor(-56.1659)\n",
      "60 0.8947482142857143 tensor(55.0812) tensor(-56.3032)\n",
      "70 0.8769089285714285 tensor(55.0810) tensor(-56.4983)\n",
      "80 0.8590696428571428 tensor(55.0807) tensor(-56.5660)\n",
      "90 0.8412303571428571 tensor(55.0803) tensor(-56.6580)\n",
      "100 0.8233910714285715 tensor(55.0800) tensor(-56.6465)\n",
      "110 0.8055517857142858 tensor(55.0798) tensor(-56.6865)\n",
      "120 0.7877125 tensor(55.0797) tensor(-56.7234)\n",
      "130 0.7698732142857143 tensor(55.0796) tensor(-56.6969)\n",
      "140 0.7520339285714286 tensor(55.0796) tensor(-56.6508)\n",
      "150 0.7341946428571429 tensor(55.0795) tensor(-56.6913)\n",
      "160 0.7163553571428571 tensor(55.0795) tensor(-56.6904)\n",
      "170 0.6985160714285714 tensor(55.0795) tensor(-56.6797)\n",
      "180 0.6806767857142857 tensor(55.0795) tensor(-56.6733)\n",
      "190 0.6628375 tensor(55.0795) tensor(-56.6933)\n",
      "200 0.6449982142857142 tensor(55.0794) tensor(-56.6791)\n",
      "210 0.6271589285714285 tensor(55.0794) tensor(-56.7211)\n",
      "220 0.6093196428571428 tensor(55.0794) tensor(-56.7016)\n",
      "230 0.5914803571428571 tensor(55.0794) tensor(-56.7010)\n",
      "240 0.5736410714285713 tensor(55.0794) tensor(-56.6909)\n",
      "250 0.5558017857142856 tensor(55.0794) tensor(-56.6583)\n",
      "260 0.5379624999999999 tensor(55.0794) tensor(-56.6934)\n",
      "270 0.5201232142857143 tensor(55.0794) tensor(-56.6957)\n",
      "280 0.5022839285714286 tensor(55.0794) tensor(-56.7120)\n",
      "290 0.48444464285714284 tensor(55.0794) tensor(-56.7130)\n",
      "300 0.4666053571428571 tensor(55.0794) tensor(-56.6857)\n",
      "310 0.4487660714285714 tensor(55.0794) tensor(-56.7063)\n",
      "320 0.43092678571428567 tensor(55.0794) tensor(-56.6882)\n",
      "330 0.41308749999999994 tensor(55.0794) tensor(-56.6686)\n",
      "340 0.3952482142857142 tensor(55.0794) tensor(-56.6782)\n",
      "350 0.3774089285714285 tensor(55.0794) tensor(-56.7607)\n",
      "360 0.3595696428571429 tensor(55.0794) tensor(-56.6913)\n",
      "370 0.34173035714285716 tensor(55.0794) tensor(-56.6753)\n",
      "380 0.32389107142857143 tensor(55.0794) tensor(-56.6806)\n",
      "390 0.3060517857142857 tensor(55.0794) tensor(-56.7167)\n",
      "400 0.2882125 tensor(55.0794) tensor(-56.6973)\n",
      "410 0.27037321428571426 tensor(55.0794) tensor(-56.7775)\n",
      "420 0.25253392857142853 tensor(55.0794) tensor(-56.7056)\n",
      "430 0.2346946428571428 tensor(55.0794) tensor(-56.6684)\n",
      "440 0.21685535714285709 tensor(55.0794) tensor(-56.7156)\n",
      "450 0.19901607142857136 tensor(55.0794) tensor(-56.7153)\n",
      "460 0.18117678571428564 tensor(55.0794) tensor(-56.7168)\n",
      "470 0.1633374999999999 tensor(55.0794) tensor(-56.7356)\n",
      "480 0.1454982142857143 tensor(55.0794) tensor(-56.7147)\n",
      "490 0.12765892857142858 tensor(55.0794) tensor(-56.7314)\n",
      "500 0.10981964285714285 tensor(55.0794) tensor(-56.7034)\n",
      "510 0.09198035714285713 tensor(55.0794) tensor(-56.7094)\n",
      "520 0.0741410714285714 tensor(55.0794) tensor(-56.7319)\n",
      "530 0.05630178571428568 tensor(55.0794) tensor(-56.7136)\n",
      "540 0.038462499999999955 tensor(55.0794) tensor(-56.7079)\n",
      "550 0.02062321428571423 tensor(55.0794) tensor(-56.7550)\n",
      "560 0.0027839285714285067 tensor(55.0794) tensor(-56.7222)\n",
      "570 0.001 tensor(55.0794) tensor(-56.7241)\n",
      "580 0.001 tensor(55.0794) tensor(-56.7154)\n",
      "590 0.001 tensor(55.0794) tensor(-56.7216)\n",
      "600 0.001 tensor(55.0793) tensor(-56.7020)\n",
      "610 0.001 tensor(55.0793) tensor(-56.6839)\n",
      "620 0.001 tensor(55.0793) tensor(-56.6827)\n",
      "630 0.001 tensor(55.0793) tensor(-56.6903)\n",
      "640 0.001 tensor(55.0793) tensor(-56.7509)\n",
      "650 0.001 tensor(55.0793) tensor(-56.6602)\n",
      "660 0.001 tensor(55.0793) tensor(-56.7015)\n",
      "670 0.001 tensor(55.0793) tensor(-56.6740)\n",
      "680 0.001 tensor(55.0793) tensor(-56.7319)\n",
      "690 0.001 tensor(55.0793) tensor(-56.6897)\n",
      "700 0.001 tensor(55.0793) tensor(-56.7172)\n",
      "{'best_eta': tensor(0.), 'best_assignment': tensor([[6.9023e-08, 1.0000e+00],\n",
      "        [2.4401e-08, 1.0000e+00],\n",
      "        [1.1181e-07, 1.0000e+00],\n",
      "        [1.6919e-08, 1.0000e+00],\n",
      "        [2.4883e-06, 1.0000e+00],\n",
      "        [8.1177e-01, 1.8823e-01],\n",
      "        [1.0000e+00, 1.5636e-07],\n",
      "        [1.0000e+00, 9.9241e-08],\n",
      "        [1.0000e+00, 2.3048e-07],\n",
      "        [1.0000e+00, 8.3595e-08]]), 'best_params': tensor([[-7.6014,  8.8875],\n",
      "        [-8.0329,  9.4958],\n",
      "        [-6.3804,  9.6260],\n",
      "        [-7.9093,  9.9855],\n",
      "        [-5.9243,  6.9796],\n",
      "        [ 3.9013,  2.4398],\n",
      "        [ 9.8613, -5.8098],\n",
      "        [ 8.7983, -7.3274],\n",
      "        [ 8.0692, -7.2139],\n",
      "        [ 8.3452, -7.9521]]), 'best_log_prob': tensor(-55.0794), 'params': tensor([[-6.5526,  8.1735],\n",
      "        [-7.3099,  8.4335],\n",
      "        [-6.2939,  9.2245],\n",
      "        [-6.9203,  8.6855],\n",
      "        [-5.7611,  7.0029],\n",
      "        [ 1.7652,  0.2818],\n",
      "        [ 8.6531, -6.9647],\n",
      "        [ 8.6348, -6.9342],\n",
      "        [ 8.9005, -6.8021],\n",
      "        [ 8.4820, -6.9080]])}\n",
      "tensor([[6.9023e-08, 1.0000e+00],\n",
      "        [2.4401e-08, 1.0000e+00],\n",
      "        [1.1181e-07, 1.0000e+00],\n",
      "        [1.6919e-08, 1.0000e+00],\n",
      "        [2.4883e-06, 1.0000e+00],\n",
      "        [8.1177e-01, 1.8823e-01],\n",
      "        [1.0000e+00, 1.5636e-07],\n",
      "        [1.0000e+00, 9.9241e-08],\n",
      "        [1.0000e+00, 2.3048e-07],\n",
      "        [1.0000e+00, 8.3595e-08]])\n",
      "tensor([[4.0231e-07, 1.0000e+00],\n",
      "        [1.4545e-07, 1.0000e+00],\n",
      "        [1.8216e-07, 1.0000e+00],\n",
      "        [1.6691e-07, 1.0000e+00],\n",
      "        [2.8620e-06, 1.0000e+00],\n",
      "        [8.1510e-01, 1.8490e-01],\n",
      "        [1.0000e+00, 1.6493e-07],\n",
      "        [1.0000e+00, 1.7317e-07],\n",
      "        [1.0000e+00, 1.5151e-07],\n",
      "        [1.0000e+00, 2.0712e-07]])\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.8118\t0.1882\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if i < 5 and j >= 5:\n",
    "            continue\n",
    "        if i >= 5 and j < 5:\n",
    "            continue\n",
    "        edges.append([i, j])\n",
    "edges.append([5, 4])\n",
    "edges.append([4, 5])\n",
    "g = nx.DiGraph()\n",
    "g.add_edges_from(edges)\n",
    "\n",
    "edges = list(g.to_directed().edges())\n",
    "edges = [(u, v, 1) for u, v in edges]\n",
    "edges = torch.tensor(edges, requires_grad=False)\n",
    "\n",
    "in_degrees = torch.tensor([float(g.to_directed().in_degree[v]) for v in g.nodes()])\n",
    "out_degrees = torch.tensor([float(g.to_directed().out_degree[v]) for v in g.nodes()])\n",
    "\n",
    "constants = {\n",
    "    'edges': edges,\n",
    "    'in_degrees': in_degrees,\n",
    "    'out_degrees': out_degrees\n",
    "}\n",
    "constants = {\n",
    "    'edges': edges\n",
    "}\n",
    "\n",
    "\n",
    "embeddings = learned_embeddings(\n",
    "    constants=constants,\n",
    "    population_size=10000,\n",
    "    k=2,\n",
    "    num_nodes=g.number_of_nodes(),\n",
    "    driver=overlapping.driver,#degcor.driver,\n",
    "    make_membership=sample_propensities,#sample_placements,\n",
    "    lr=1,\n",
    "    num_iter=700\n",
    ")\n",
    "\n",
    "print(embeddings)\n",
    "print(torch.softmax(embeddings['best_params'], -1))\n",
    "print(torch.softmax(embeddings['params'], -1))\n",
    "\n",
    "members = torch.clamp(torch.softmax(embeddings['best_params'], -1), 0.001, 0.999).numpy()\n",
    "for i in range(members.shape[0]):\n",
    "    for j in range(members.shape[1]):\n",
    "        print('{:.4f}'.format(members[i, j]), end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished iteration # 0\n",
      "Finished iteration # 1\n",
      "Finished iteration # 2\n",
      "Finished iteration # 3\n",
      "Finished iteration # 4\n",
      "Finished iteration # 5\n",
      "Finished iteration # 6\n",
      "Finished iteration # 7\n",
      "Finished iteration # 8\n",
      "Finished iteration # 9\n",
      "\n",
      "Mean Duration until Convergence:  7.963886952400207\n"
     ]
    }
   ],
   "source": [
    "# Runtime Comparison: General\n",
    "def get_mean_runtime(num_iters):\n",
    "    durs = []\n",
    "    for i in range(num_iters):\n",
    "        with suppress_stdout():\n",
    "            start = time.time()\n",
    "            embeddings = learned_embeddings(\n",
    "                constants=constants,\n",
    "                population_size=4000,\n",
    "                k=2,\n",
    "                num_nodes=g.number_of_nodes(),\n",
    "                driver=overlapping.driver,#degcor.driver,\n",
    "                make_membership=sample_propensities,#sample_placements,\n",
    "                lr=1,\n",
    "                num_iter=600\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "        duration = end - start \n",
    "        durs.append(duration)\n",
    "        print(\"Finished iteration #\", i)\n",
    "    \n",
    "    mean_dur = sum(durs)/num_iters\n",
    "    \n",
    "    return mean_dur, durs\n",
    "\n",
    "\n",
    "mean_dur, durs = get_mean_runtime(10)\n",
    "print(\"\\nMean Duration until Convergence: \", mean_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished iteration # 0\nFinished iteration # 1\nFinished iteration # 2\nFinished iteration # 3\nFinished iteration # 4\nFinished iteration # 5\nFinished iteration # 6\nFinished iteration # 7\nFinished iteration # 8\nFinished iteration # 9\n\nMean Duration for one Iteration:  0.010897254943847657\n\nMean Iters/s:  92\n"
     ]
    }
   ],
   "source": [
    "# Runtime Comparison: Iterative\n",
    "def get_mean_runtime(num_iters):\n",
    "    durs = []\n",
    "    for i in range(num_iters):\n",
    "        with suppress_stdout():\n",
    "            start = time.time()\n",
    "            embeddings = learned_embeddings(\n",
    "                constants=constants,\n",
    "                population_size=4000,\n",
    "                k=2,\n",
    "                num_nodes=g.number_of_nodes(),\n",
    "                driver=overlapping.driver,#degcor.driver,\n",
    "                make_membership=sample_propensities,#sample_placements,\n",
    "                lr=1,\n",
    "                num_iter=1\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "        duration = end - start \n",
    "        durs.append(duration)\n",
    "        print(\"Finished iteration #\", i)\n",
    "    \n",
    "    mean_dur = sum(durs)/num_iters\n",
    "    \n",
    "    return mean_dur, durs\n",
    "\n",
    "\n",
    "mean_dur, durs = get_mean_runtime(10)\n",
    "print(\"\\nMean Duration for one Iteration: \", mean_dur)\n",
    "print(\"\\nMean Iters/s: \", round(1/mean_dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}