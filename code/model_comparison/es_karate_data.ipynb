{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU used:  True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/djuhera/es-sbm-master')\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import vis\n",
    "import degcor\n",
    "import overlapping\n",
    "import bernoulli\n",
    "import time\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = 'cpu'\n",
    "\n",
    "# Suppress Console Output\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            \n",
    "\n",
    "def make_utilities(population_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Make utility values to weight samples with.\n",
    "\n",
    "    Args:\n",
    "        population_size:\n",
    "\n",
    "    Returns:\n",
    "        utility_values: Shape (S,).\n",
    "    \"\"\"\n",
    "    utility_values = np.log(population_size / 2. + 1) - \\\n",
    "                     np.log(np.arange(1, population_size + 1))\n",
    "    utility_values[utility_values < 0] = 0.\n",
    "    utility_values /= np.sum(utility_values)\n",
    "    utility_values -= 1. / population_size\n",
    "    utility_values = utility_values.astype(np.float32)\n",
    "    return torch.tensor(utility_values)\n",
    "\n",
    "\n",
    "def scale_noise(noise: torch.Tensor, fitness: torch.Tensor, log_probs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Scale noise with the fitness values. Fitness values are assigned to noise\n",
    "    vectors based on the `log_probs` vector. The better the `log_probs` vector,\n",
    "    the higher the fitness and thus the more the gradient should point into\n",
    "    that direction.\n",
    "\n",
    "    Args:\n",
    "        noise: Noise used to perturb the parameters (S, |V|, k).\n",
    "        fitness: Vector of scalars, first element `fitness[0]` is the fitness corresponding\n",
    "            to the best sample, the last element `fitness[-1] is the fitness\n",
    "            for the worst sample. Has shape (S).\n",
    "        log_probs: Log prob resulting from the sampled placements. Has shape (S,).\n",
    "\n",
    "    Returns:\n",
    "        scaled_noise: Noise scaled with fitness values. Has shape (S, |V|, d).\n",
    "    \"\"\"\n",
    "    sorted_values, indices = torch.sort(-1. * log_probs)\n",
    "    scaled_noise = fitness.unsqueeze(1).unsqueeze(2) * noise[indices, :, :]\n",
    "    return scaled_noise\n",
    "\n",
    "\n",
    "def apply_gradient(params: torch.Tensor, grads: torch.Tensor, lr: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply the gradient to the parameters.\n",
    "\n",
    "    Args:\n",
    "        params: Tensor with parameters of shape (|V|, d).\n",
    "        grads: Tensor with gradients of shape (|V|, d).\n",
    "\n",
    "    Returns:\n",
    "        params_prime: Tensor of shape (|V|, d).\n",
    "    \"\"\"\n",
    "    params_prime = params + lr * grads\n",
    "    return params_prime\n",
    "\n",
    "\n",
    "def calculate_gradient(scaled_noise: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calcualte the gradient from the scaled noise.\n",
    "\n",
    "    Args:\n",
    "        scaled_noise: Tensor of shape (S, |V|, d).\n",
    "\n",
    "    Returns:\n",
    "        grad: Tensor of shape (|V|, d).\n",
    "    \"\"\"\n",
    "    grad = torch.sum(scaled_noise, 0)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sample_placements(mutated_params: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample indices from mutated paramters.\n",
    "\n",
    "    Args:\n",
    "        mutated_params: Shape (S, |V|, k)\n",
    "\n",
    "    Returns:\n",
    "        memberships: Shape (S, |V|).\n",
    "    \"\"\"\n",
    "    # cumsum = torch.cumsum(torch.softmax(mutated_params, dim=-1), dim=-1)\n",
    "    # noise = torch.rand(mutated_params.shape[0], mutated_params.shape[1], 1, requires_grad=False)\n",
    "    # indices = torch.argmin((cumsum < noise).int(), dim=-1)\n",
    "    indices = torch.distributions.Categorical(logits=mutated_params).sample()\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_propensities(mutated_params: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create propensities for parameters, i.e., soft assignment.\n",
    "\n",
    "    Args:\n",
    "        mutated_params: Shape (S, |V|, k)\n",
    "\n",
    "    Returns:\n",
    "        memberships: Shape (S, |V|, k).\n",
    "    \"\"\"\n",
    "    return torch.softmax(mutated_params, -1)\n",
    "\n",
    "\n",
    "def mutate_params(params: torch.Tensor, num_samples: int, scale: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create `num_samples` mutation of parameters.\n",
    "\n",
    "    Args:\n",
    "        params: Shape (|V|, k)\n",
    "\n",
    "    Returns:\n",
    "        mutated_params: Shape (S, |V|, k)\n",
    "    \"\"\"\n",
    "    noise = torch.randn([num_samples, params.shape[0], params.shape[1]], requires_grad=False)\n",
    "    mutated_params = torch.unsqueeze(params, 0) + noise\n",
    "    return noise, mutated_params\n",
    "\n",
    "\n",
    "def calc_scale_decrease(num_iter: int, fraction: float, target_scale: float) -> float:\n",
    "    stop = num_iter * fraction\n",
    "    m = (target_scale - 1.) / stop\n",
    "    return m\n",
    "\n",
    "\n",
    "def learned_embeddings(constants: Dict[str, torch.Tensor], population_size: int, k: int,\n",
    "                       num_nodes: int, driver: callable, make_membership: callable,\n",
    "                       lr: float, num_iter=10000, params: torch.Tensor=None,\n",
    "                       patience: int=None) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        constants: Dictionary containing constant arguments to driver functions.\n",
    "            For example, edges, in_degrees, out_degrees. Check the signature call\n",
    "            of a specific driver function to determine the fields of this dict.\n",
    "        population_size: Number of samples for ES.\n",
    "        k: Number of groups.\n",
    "        num_nodes: Number of nodes in graph.\n",
    "        driver: Callable that implements the calculation of the log-prob and\n",
    "            estimates parameters of a model.\n",
    "        lr: Learning rate that should be used.\n",
    "        num_iter: Number of iterations that should be executed.\n",
    "        params: Initial guess of parameters, if None start from uniform distribution\n",
    "            over groups.\n",
    "        patience: Extension of learning after new best log prob has been found.\n",
    "\n",
    "    Returns:\n",
    "        Dict\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        # Start from a uniform distribution over group memberships.\n",
    "        params = torch.ones(num_nodes, k, requires_grad=False, device=DEV)\n",
    "    fitness = make_utilities(population_size)\n",
    "\n",
    "    target_scale = 1e-3\n",
    "    fraction = 0.8\n",
    "    ascend = calc_scale_decrease(num_iter, fraction, target_scale)\n",
    "    all_losses = []\n",
    "    best_loss = 1e9\n",
    "    best_params = None\n",
    "    best_eta = None\n",
    "    best_assignment = None\n",
    "    patience = int(num_iter * 0.1) if patience is None else patience\n",
    "    iter = 0\n",
    "    while iter < num_iter:\n",
    "        scale = float(np.clip(iter * ascend + 1, target_scale, 1.))\n",
    "        # scale = 1.\n",
    "        iter += 1\n",
    "        noise, mutations = mutate_params(params, population_size, scale)\n",
    "        assignments = make_membership(mutations)\n",
    "        log_probs, estimates = driver(assignments, k, **constants)\n",
    "        all_losses.append(log_probs.cpu().detach().numpy())\n",
    "\n",
    "        bl = torch.min(-1. * log_probs)\n",
    "        if bl <= best_loss:\n",
    "            best_loss = bl.cpu().detach()\n",
    "            idx = torch.argmax(log_probs)\n",
    "            best_params = mutations[idx, :, :]\n",
    "            best_eta = estimates['etas'][idx]\n",
    "            best_assignment = assignments[idx]\n",
    "            if iter + patience > num_iter and bl < best_loss:\n",
    "                num_iter = iter + patience\n",
    "        if iter % 10 == 0:\n",
    "            print(iter, scale,  best_loss, torch.mean(log_probs))\n",
    "\n",
    "        scaled_noise = scale_noise(noise, fitness, log_probs)\n",
    "        grads = calculate_gradient(scaled_noise)\n",
    "        params = apply_gradient(params, grads, lr)\n",
    "\n",
    "    return {\n",
    "        \"best_eta\": best_eta.cpu(),\n",
    "        \"best_assignment\": best_assignment.cpu(),\n",
    "        \"best_params\": best_params.cpu(),\n",
    "        \"best_log_prob\": -1. * bl,\n",
    "        \"params\": params.cpu()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10 0.98126875 tensor(668.8786) tensor(-696.9060)\n",
      "20 0.96045625 tensor(628.8748) tensor(-648.9282)\n",
      "30 0.93964375 tensor(619.0401) tensor(-630.2614)\n",
      "40 0.91883125 tensor(617.7997) tensor(-627.9886)\n",
      "50 0.89801875 tensor(617.2100) tensor(-627.4258)\n",
      "60 0.87720625 tensor(617.0930) tensor(-627.1802)\n",
      "70 0.85639375 tensor(616.8852) tensor(-627.0475)\n",
      "80 0.8355812499999999 tensor(616.7005) tensor(-626.9092)\n",
      "90 0.81476875 tensor(616.5712) tensor(-626.8587)\n",
      "100 0.7939562499999999 tensor(616.5712) tensor(-626.7922)\n",
      "110 0.77314375 tensor(616.5712) tensor(-626.7832)\n",
      "120 0.75233125 tensor(616.5635) tensor(-626.6899)\n",
      "130 0.73151875 tensor(616.5635) tensor(-626.7875)\n",
      "140 0.71070625 tensor(616.5635) tensor(-626.6567)\n",
      "150 0.68989375 tensor(616.5411) tensor(-626.7739)\n",
      "160 0.66908125 tensor(616.4867) tensor(-626.7245)\n",
      "170 0.64826875 tensor(616.4867) tensor(-626.7720)\n",
      "180 0.6274562499999999 tensor(616.4545) tensor(-626.7566)\n",
      "190 0.6066437499999999 tensor(616.4545) tensor(-626.6299)\n",
      "200 0.58583125 tensor(616.4545) tensor(-626.6277)\n",
      "210 0.5650187499999999 tensor(616.4545) tensor(-626.6118)\n",
      "220 0.54420625 tensor(616.4106) tensor(-626.7276)\n",
      "230 0.5233937499999999 tensor(616.4106) tensor(-626.6448)\n",
      "240 0.50258125 tensor(616.3675) tensor(-626.6490)\n",
      "250 0.48176874999999997 tensor(616.3675) tensor(-626.6827)\n",
      "260 0.46095624999999996 tensor(616.3675) tensor(-626.5695)\n",
      "270 0.44014374999999994 tensor(616.3675) tensor(-626.7025)\n",
      "280 0.41933124999999993 tensor(616.3675) tensor(-626.6117)\n",
      "290 0.3985187499999999 tensor(616.3675) tensor(-626.6851)\n",
      "300 0.3777062499999999 tensor(616.3675) tensor(-626.7098)\n",
      "310 0.3568937499999999 tensor(616.3675) tensor(-626.5250)\n",
      "320 0.3360812499999999 tensor(616.3675) tensor(-626.6968)\n",
      "330 0.31526875 tensor(616.2940) tensor(-626.6943)\n",
      "340 0.29445625 tensor(616.2940) tensor(-626.5704)\n",
      "350 0.27364374999999996 tensor(616.2940) tensor(-626.5420)\n",
      "360 0.25283124999999995 tensor(616.2940) tensor(-626.6415)\n",
      "370 0.23201874999999994 tensor(616.2940) tensor(-626.6082)\n",
      "380 0.21120624999999993 tensor(616.2940) tensor(-626.5792)\n",
      "390 0.19039374999999992 tensor(616.2940) tensor(-626.5155)\n",
      "400 0.1695812499999999 tensor(616.2940) tensor(-626.6123)\n",
      "410 0.1487687499999999 tensor(616.2940) tensor(-626.5760)\n",
      "420 0.12795624999999988 tensor(616.2940) tensor(-626.5998)\n",
      "430 0.10714374999999987 tensor(616.2785) tensor(-626.5553)\n",
      "440 0.08633124999999986 tensor(616.2456) tensor(-626.6833)\n",
      "450 0.06551874999999996 tensor(616.2456) tensor(-626.6452)\n",
      "460 0.04470624999999995 tensor(616.2390) tensor(-626.5583)\n",
      "470 0.023893749999999936 tensor(616.2390) tensor(-626.5837)\n",
      "480 0.0030812499999999243 tensor(616.2390) tensor(-626.5236)\n",
      "490 0.001 tensor(616.2390) tensor(-626.5978)\n",
      "500 0.001 tensor(616.2390) tensor(-626.5372)\n",
      "510 0.001 tensor(616.2390) tensor(-626.4917)\n",
      "520 0.001 tensor(616.2390) tensor(-626.6381)\n",
      "530 0.001 tensor(616.2390) tensor(-626.5432)\n",
      "540 0.001 tensor(616.2390) tensor(-626.4777)\n",
      "550 0.001 tensor(616.2390) tensor(-626.6754)\n",
      "560 0.001 tensor(616.2390) tensor(-626.5553)\n",
      "570 0.001 tensor(616.2390) tensor(-626.5258)\n",
      "580 0.001 tensor(616.2092) tensor(-626.5484)\n",
      "590 0.001 tensor(616.2092) tensor(-626.4918)\n",
      "600 0.001 tensor(616.2092) tensor(-626.5540)\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.3949\t0.6051\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.5374\t0.4626\t\n",
      "0.7330\t0.2670\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.1895\t0.8105\t\n",
      "0.9990\t0.0010\t\n",
      "0.9916\t0.0084\t\n",
      "0.0010\t0.9990\t\n",
      "0.0010\t0.9990\t\n",
      "0.9987\t0.0013\t\n",
      "0.2632\t0.7368\t\n",
      "0.9990\t0.0010\t\n",
      "0.0010\t0.9990\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n",
      "0.9956\t0.0044\t\n",
      "0.9982\t0.0018\t\n",
      "0.5446\t0.4554\t\n",
      "0.7855\t0.2145\t\n",
      "0.9990\t0.0010\t\n",
      "0.9990\t0.0010\t\n"
     ]
    }
   ],
   "source": [
    "g = nx.karate_club_graph()\n",
    "edges = list(g.to_directed().edges())\n",
    "edges = [(u, v, 1) for u, v in edges]\n",
    "edges = torch.tensor(edges, requires_grad=False)\n",
    "\n",
    "in_degrees = torch.tensor([float(g.to_directed().in_degree[v]) for v in g.nodes()])\n",
    "out_degrees = torch.tensor([float(g.to_directed().out_degree[v]) for v in g.nodes()])\n",
    "\n",
    "constants = {\n",
    "    'edges': edges,\n",
    "    'in_degrees': in_degrees,\n",
    "    'out_degrees': out_degrees\n",
    "}\n",
    "constants = {\n",
    "    'edges': edges\n",
    "}\n",
    "\n",
    "embeddings = learned_embeddings(\n",
    "    constants=constants,\n",
    "    population_size=10000,\n",
    "    k=2,\n",
    "    num_nodes=g.number_of_nodes(),\n",
    "    driver=overlapping.driver,#degcor.driver,\n",
    "    make_membership=sample_propensities,#sample_placements,\n",
    "    lr=1,\n",
    "    num_iter=600\n",
    ")\n",
    "\n",
    "#print(embeddings)\n",
    "#print(torch.softmax(embeddings['best_params'], -1))\n",
    "#print(torch.softmax(embeddings['params'], -1))\n",
    "\n",
    "members = torch.clamp(torch.softmax(embeddings['best_params'], -1), 0.001, 0.999).numpy()\n",
    "for i in range(members.shape[0]):\n",
    "    for j in range(members.shape[1]):\n",
    "        print('{:.4f}'.format(members[i, j]), end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished iteration # 0\n",
      "Finished iteration # 1\n",
      "Finished iteration # 2\n",
      "Finished iteration # 3\n",
      "Finished iteration # 4\n",
      "Finished iteration # 5\n",
      "Finished iteration # 6\n",
      "Finished iteration # 7\n",
      "Finished iteration # 8\n",
      "Finished iteration # 9\n",
      "\n",
      "Mean Duration until Convergence:  14.813968777656555\n"
     ]
    }
   ],
   "source": [
    "# Runtime Comparison: General\n",
    "def get_mean_runtime(num_iters):\n",
    "    durs = []\n",
    "    for i in range(num_iters):\n",
    "        with suppress_stdout():\n",
    "            start = time.time()\n",
    "            embeddings = learned_embeddings(\n",
    "                constants=constants,\n",
    "                population_size=4000,\n",
    "                k=2,\n",
    "                num_nodes=g.number_of_nodes(),\n",
    "                driver=overlapping.driver,#degcor.driver,\n",
    "                make_membership=sample_propensities,#sample_placements,\n",
    "                lr=1,\n",
    "                num_iter=600\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "        duration = end - start \n",
    "        durs.append(duration)\n",
    "        print(\"Finished iteration #\", i)\n",
    "    \n",
    "    mean_dur = sum(durs)/num_iters\n",
    "    \n",
    "    return mean_dur, durs\n",
    "\n",
    "\n",
    "mean_dur, durs = get_mean_runtime(10)\n",
    "print(\"\\nMean Duration until Convergence: \", mean_dur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished iteration # 0\n",
      "Finished iteration # 1\n",
      "Finished iteration # 2\n",
      "Finished iteration # 3\n",
      "Finished iteration # 4\n",
      "Finished iteration # 5\n",
      "Finished iteration # 6\n",
      "Finished iteration # 7\n",
      "Finished iteration # 8\n",
      "Finished iteration # 9\n",
      "\n",
      "Mean Duration for one Iteration:  0.028452515602111816\n",
      "\n",
      "Mean Iters/s:  35\n"
     ]
    }
   ],
   "source": [
    "# Runtime Comparison: Iterative\n",
    "def get_mean_runtime(num_iters):\n",
    "    durs = []\n",
    "    for i in range(num_iters):\n",
    "        with suppress_stdout():\n",
    "            start = time.time()\n",
    "            embeddings = learned_embeddings(\n",
    "                constants=constants,\n",
    "                population_size=4000,\n",
    "                k=2,\n",
    "                num_nodes=g.number_of_nodes(),\n",
    "                driver=overlapping.driver,#degcor.driver,\n",
    "                make_membership=sample_propensities,#sample_placements,\n",
    "                lr=1,\n",
    "                num_iter=1\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "        duration = end - start \n",
    "        durs.append(duration)\n",
    "        print(\"Finished iteration #\", i)\n",
    "    \n",
    "    mean_dur = sum(durs)/num_iters\n",
    "    \n",
    "    return mean_dur, durs\n",
    "\n",
    "\n",
    "mean_dur, durs = get_mean_runtime(10)\n",
    "print(\"\\nMean Duration for one Iteration: \", mean_dur)\n",
    "print(\"\\nMean Iters/s: \", round(1/mean_dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}