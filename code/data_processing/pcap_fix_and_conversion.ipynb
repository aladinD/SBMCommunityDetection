{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from typing import List\n",
    "import datetime\n",
    "import h5py\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# Get Time Sorted PCAP List\n",
    "def get_pcap_list(pcap_dir):\n",
    "    # Capture Files in Time Sorted List \n",
    "    pcap_files = []\n",
    "    for file in sorted(glob.glob(\"{}/{}\".format(pcap_dir, \"*.pcap\"))):\n",
    "        pcap_files.append(file)\n",
    "\n",
    "    # Return \n",
    "    return pcap_files\n",
    "\n",
    "\n",
    "# Merge IPv4 and IPv6 SRC and DST Columns \n",
    "def correct_ipv6(df):\n",
    "    # Locations with NaN Values\n",
    "    locs = df['ip.src'].isna()\n",
    "\n",
    "    # Replace Values\n",
    "    df.loc[locs, 'ip.src'] = df.loc[locs, 'ipv6.src']\n",
    "    df.loc[locs, 'ip.dst'] = df.loc[locs, 'ipv6.dst']\n",
    "\n",
    "    # Delete ipv6 columns\n",
    "    del df['ipv6.src']\n",
    "    del df['ipv6.dst']\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Merge UDP and TCP SRC and DST Ports\n",
    "def correct_UDP_TCP_ports(df):\n",
    "    # Locations with NaN Values\n",
    "    locs = df['udp.srcport'].isna()\n",
    "\n",
    "    # Replace Values\n",
    "    df.loc[locs, 'udp.srcport'] = df.loc[locs, 'tcp.srcport']\n",
    "    df.loc[locs, 'udp.dstport'] = df.loc[locs, 'tcp.dstport']\n",
    "\n",
    "    # Delete TCP Columns\n",
    "    del df['tcp.srcport']\n",
    "    del df['tcp.dstport']\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "\n",
    "\n",
    "# Mask SRC and DST Ports larger than 1024 with value 66000\n",
    "def port_masking(df):\n",
    "    # Assign Masked SRC and DST Port Values in New Column\n",
    "    df['Masked Source Port'] = 66000\n",
    "    df['Masked Destination Port'] = 66000\n",
    "\n",
    "    # Mask SRC Ports\n",
    "    mask = df['udp.srcport'] < 1025\n",
    "    df.loc[mask, 'Masked Source Port'] = df.loc[mask, 'udp.srcport']\n",
    "\n",
    "    # Mask DST Ports\n",
    "    mask = df['udp.dstport'] < 1025\n",
    "    df.loc[mask, 'Masked Destination Port'] = df.loc[mask, 'udp.dstport']\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add Number of Packets to the DataFrame\n",
    "def add_num_packets(df):\n",
    "    # Retrieve Number of Packets\n",
    "    num_packets = len(df)\n",
    "\n",
    "    # Add Column \n",
    "    df['Number of Packets'] = num_packets\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add Trace Duration Info to the DataFrame\n",
    "def add_duration_time(df):\n",
    "    # Compute Duration\n",
    "    duration = df.iloc[len(df)-1, df.columns.get_loc('frame.time_epoch')] - df.iloc[0, df.columns.get_loc('frame.time_epoch')]\n",
    "\n",
    "    # Add Column\n",
    "    df['Trace Duration [s]'] = duration\n",
    "\n",
    "    # Return \n",
    "    return df\n",
    "    \n",
    "\n",
    "# Rename the DataFrame \n",
    "def rename_dataframe(df):\n",
    "    # Rename Columns\n",
    "    df.rename(columns = {'frame.number': 'No.', 'frame.time_epoch': 'Time Epoch', 'frame.time': 'Packet Arrival Time', 'ip.src': 'Source IP', 'ip.dst': 'Destination                           IP', '_ws.col.Protocol': 'Protocol', 'frame.len': 'Traffic Size [Byte]', 'udp.srcport': 'Source Port', 'udp.dstport': 'Destination Port'},                           inplace = True)\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Rearrange DataFrame Columns \n",
    "def rearrange_dataframe(df):\n",
    "    # New Columns\n",
    "    new_cols = ['No.', 'Time Epoch', 'Packet Arrival Time', 'Trace Duration [s]', 'Number of Packets', 'Source IP', 'Destination                           IP', 'Protocol', 'Traffic Size [Byte]', 'Source Port', 'Destination Port', 'Masked Source Port', 'Masked Destination Port']\n",
    "\n",
    "    # Rearrange\n",
    "    df = df[new_cols]\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "\n",
    "# Convert Pandas DataFrames to HDF5 \n",
    "def convert_df_to_HDF(df, csv_file, csv_dir, hdf_dir):\n",
    "    # File Directories \n",
    "    csv_dir = csv_dir + '/'\n",
    "\n",
    "    # File Naming\n",
    "    csv_name = csv_file.split(csv_dir)[1]\n",
    "    file_name = csv_name.split('.csv')[0]\n",
    "\n",
    "    # File Pathing\n",
    "    hdf_path = hdf_dir + file_name + ('.h5')\n",
    "\n",
    "    # Create HDF5 File\n",
    "    hdf = pd.HDFStore(hdf_path)\n",
    "\n",
    "    # Store Pandas Frame\n",
    "    hdf.put('PCAP', df)\n",
    "\n",
    "    # Close HDF5 File\n",
    "    hdf.close()\n",
    "\n",
    "\n",
    "# Rename CSV Files that Do Not Follow the Naming Convention of the Directory\n",
    "def rename_rogue_csv_files(csv_path):\n",
    "    # Get Rogue CSV Files\n",
    "    rogue_csv_files = []\n",
    "    _, _, filepath = next(os.walk(csv_path))\n",
    "\n",
    "    for file in filepath:\n",
    "        filename = csv_path + '/' + file\n",
    "        keyword = '/capture'\n",
    "        if keyword in filename:\n",
    "            rogue_csv_files.append(filename)\n",
    "\n",
    "    # Rename According to Epoch\n",
    "    counter = 0\n",
    "    for i in range(len(rogue_csv_files)):\n",
    "        df = pd.read_csv(rogue_csv_files[i])\n",
    "        epoch = int(df.loc[0, 'Time Epoch'])\n",
    "\n",
    "        old_path = rogue_csv_files[i]\n",
    "        old_name = old_path.split(csv_path + '/')[1]\n",
    "        new_path = csv_path + '/' + str(epoch) + '-' + old_name\n",
    "\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    print(\"Successfully renamed {} rogue files.\".format(counter))\n",
    "\n",
    "\n",
    "# Fix Broken PCAP Files\n",
    "def apply_pcap_fix(pcap_file_path: str, pcap_file: str):\n",
    "    # Naming\n",
    "    original_name = pcap_file.split(pcap_file_path + '/')[1]\n",
    "    fixed_name = 'fixed_' + original_name\n",
    "    pcap_file_destination = pcap_file_path + '/' + original_name\n",
    "\n",
    "    # Commands\n",
    "    command1 = 'cd /home/djuhera/pcapfix-1.1.4'\n",
    "    command2 = ('/home/djuhera/pcapfix-1.1.4/pcapfix -d {}').format(pcap_file)\n",
    "    command3 = ('mv /home/djuhera/notebooks/{} {}').format(fixed_name, pcap_file_destination)\n",
    "\n",
    "    # Execution\n",
    "    subprocess.check_call(command1, shell=True)\n",
    "    subprocess.run(command2, shell=True)\n",
    "    subprocess.check_call(command3, shell=True)\n",
    "\n",
    "\n",
    "# Remove Fraudulent PCAPs from List\n",
    "def delete_fraudulent_pcaps(pcap_list, fraudulent_pcaps):\n",
    "    for i in range(len(fraudulent_pcaps)):\n",
    "        file_name = fraudulent_pcaps[i]\n",
    "        index = pcap_list.index(file_name)\n",
    "        del pcap_list[index]\n",
    "\n",
    "    return pcap_list\n",
    "\n",
    "\n",
    "# Convert PCAP Files to CSV Frames and Optionally Store as HDF5\n",
    "def convert_pcap_to_csvframe(pcap_dir: str, pcap_file: str, csv_dir: str, store_HDF5: bool):\n",
    "    # PCAP File Location\n",
    "\n",
    "    # Replace File Suffix .pcap with .csv\n",
    "    pcap_csv_rename = []\n",
    "    for files in pcap_file:\n",
    "        pcap_csv_rename.append(files.replace('pcap', 'csv'))\n",
    "\n",
    "    # Generate List with Correct Pathing and CSV Name\n",
    "    csv_file = []\n",
    "    for files in pcap_csv_rename:\n",
    "        csv_file.append(files.replace(pcap_dir, csv_dir))\n",
    "\n",
    "    # TSHARK Commands\n",
    "    broken_pcaps = 0\n",
    "    for i in range(len(csv_file)):\n",
    "        try:\n",
    "            command = ('tshark -r {} -T fields '\n",
    "                    '-e frame.number '\n",
    "                    '-e frame.time_epoch '\n",
    "                    '-e frame.time '\n",
    "                    '-e ip.src '\n",
    "                    '-e ipv6.src '\n",
    "                    '-e ip.dst '\n",
    "                    '-e ipv6.dst '\n",
    "                    '-e _ws.col.Protocol '\n",
    "                    '-e frame.len '\n",
    "                    '-e tcp.srcport '\n",
    "                    '-e tcp.dstport '\n",
    "                    '-e udp.srcport '\n",
    "                    '-e udp.dstport '\n",
    "                    '-E header=y -E separator=, -E quote=d > {}').format(\n",
    "                pcap_file[i],\n",
    "                csv_file[i]\n",
    "            )\n",
    "            subprocess.check_call(command, shell=True)\n",
    "        except: \n",
    "            print(\"Found Broken PCAP at index {}\".format(i))\n",
    "            broken_pcaps = broken_pcaps + 1\n",
    "            apply_pcap_fix(pcap_dir, pcap_file[i])\n",
    "            command = ('tshark -r {} -T fields '\n",
    "                    '-e frame.number '\n",
    "                    '-e frame.time_epoch '\n",
    "                    '-e frame.time '\n",
    "                    '-e ip.src '\n",
    "                    '-e ipv6.src '\n",
    "                    '-e ip.dst '\n",
    "                    '-e ipv6.dst '\n",
    "                    '-e _ws.col.Protocol '\n",
    "                    '-e frame.len '\n",
    "                    '-e tcp.srcport '\n",
    "                    '-e tcp.dstport '\n",
    "                    '-e udp.srcport '\n",
    "                    '-e udp.dstport '\n",
    "                    '-E header=y -E separator=, -E quote=d > {}').format(\n",
    "                pcap_file[i],\n",
    "                csv_file[i]\n",
    "            )\n",
    "            subprocess.check_call(command, shell=True)\n",
    "\n",
    "\n",
    "        # Read CSV in Pandas\n",
    "        pd_data = pd.read_csv(csv_file[i])\n",
    "\n",
    "        # Apply Modifications\n",
    "        correct_ipv6(pd_data)\n",
    "        correct_UDP_TCP_ports(pd_data)\n",
    "        port_masking(pd_data)\n",
    "        add_num_packets(pd_data)\n",
    "        add_duration_time(pd_data)\n",
    "        rename_dataframe(pd_data)\n",
    "\n",
    "        # Rearrange Columns\n",
    "        pd_data = rearrange_dataframe(pd_data)\n",
    "\n",
    "        # Convert Pandas Frame to CSV\n",
    "        pd_data.to_csv(csv_file[i], index=False)\n",
    "\n",
    "        # Convert Pandas Frame to HDF5 File\n",
    "        if store_HDF5:\n",
    "            hdf_dir = '/home/djuhera/DATA/HDF_files/'\n",
    "            convert_df_to_HDF(pd_data, csv_file[i], csv_dir, hdf_dir)\n",
    "        else:\n",
    "            None\n",
    "    \n",
    "    # Rogue File Renaming\n",
    "    rename_rogue_csv_files(csv_dir)\n",
    "\n",
    "    # Final Notice\n",
    "    print(\"Done converting {} files.\".format(len(pcap_file)))\n",
    "    print(\"Fixed {} Broken PCAPS\".format(broken_pcaps))\n",
    "    if store_HDF5:\n",
    "        print(\"Files were also stored as HDF5.\") \n",
    "    else:\n",
    "        print(\"Files were not stored as HDF5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DONE MOVING 21444 FILES.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Move Traces for Testing\n",
    "\n",
    "# Directory\n",
    "pcap_dir = \"/home/djuhera/TRACES\"\n",
    "save_dir = \"/home/djuhera/DATA/Test_Traces\"\n",
    "\n",
    "# Get PCAP Files\n",
    "pcap_list = get_pcap_list(pcap_dir)\n",
    "\n",
    "# How Many\n",
    "num_traces = len(pcap_list)\n",
    "\n",
    "for i in range(num_traces):\n",
    "    source = pcap_list[i]\n",
    "    source_name = source.split(pcap_dir + '/')[1]\n",
    "    destination = save_dir + '/' + source_name\n",
    "\n",
    "    copyfile(source, destination)\n",
    "\n",
    "print((\"DONE MOVING {} FILES.\").format(num_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Files to Convert:  21440\n"
     ]
    }
   ],
   "source": [
    "# PCAP Conversion + PCAP Fix\n",
    "pcap_dir = \"/home/djuhera/DATA/Test_Traces\"\n",
    "csv_dir = \"/home/djuhera/DATA/CSV_Testing\"\n",
    "\n",
    "# Get PCAP Files\n",
    "pcap_list = get_pcap_list(pcap_dir)\n",
    "\n",
    "# Remove Fraudulent PCAPs from List\n",
    "fraudulent_pcaps = ['/home/djuhera/DATA/Test_Traces/1597325710-capture-vmx6-9546.pcap', '/home/djuhera/DATA/Test_Traces/1597325747-capture-vmx6-9615.pcap', '/home/djuhera/DATA/Test_Traces/1597390653-capture-vmx6-16092.pcap', '/home/djuhera/DATA/Test_Traces/1597413418-capture-vmx6-18430.pcap']\n",
    "delete_fraudulent_pcaps(pcap_list, fraudulent_pcaps)\n",
    "print(\"Number of Files to Convert: \", len(pcap_list))\n",
    "\n",
    "# Conversion\n",
    "convert_pcap_to_csvframe(pcap_dir, pcap_list, csv_dir, store_HDF5=True)\n",
    "\n",
    "# Finish\n",
    "print(\"\\n -------------------------------------------------- DONE --------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}